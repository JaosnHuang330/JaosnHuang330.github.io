<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Reinforcement Learning Notes (3)</title>
      <link href="/2025/11/09/RL-note3/"/>
      <url>/2025/11/09/RL-note3/</url>
      
        <content type="html"><![CDATA[<h1 id="Monte-Carlo-Estimation"><a href="#Monte-Carlo-Estimation" class="headerlink" title="Monte Carlo Estimation"></a>Monte Carlo Estimation</h1><blockquote><p>First Introduction of <strong>Model-Free Estimation Algorithm</strong>. The dilemma of Data and Model. The <em><strong>mean estimation</strong></em> is the core of MC Estimation, which <strong>uses stochastic experience samples to solve estimation problem</strong>.</p></blockquote><h2 id="Mean-Estimation"><a href="#Mean-Estimation" class="headerlink" title="Mean Estimation"></a>Mean Estimation</h2><p>Consider the a random variable $X$ which can take values from a finite set $\mathcal X$. There are two approaches to calculate the estimation of $\mathbb{E}[X]$, model based and model-free. $\mathbb{E}[X]$ is the expected value, mean value, and average. </p><ol><li><p>model-based </p><p> $$<br> \mathbb{E}[X] &#x3D; \sum_{x \in \mathcal{X}} p(x) x<br> $$</p></li><li><p>model-free (the samples have to be iid</p><p> $$<br> \mathbb{E}[X] \approx \bar x &#x3D; \frac{1}{n} \sum_{j&#x3D;1}^{n} x_j,<br> \quad n\rightarrow\infty,\quad \bar x \rightarrow \mathbb{E}[X]<br> $$</p></li></ol><h2 id="MC-Basic"><a href="#MC-Basic" class="headerlink" title="MC Basic"></a>MC Basic</h2><p>It’s important to know the fundamental idea of MC-based Reinforcement Learning. </p><p>For model-based model, the action value is:</p><p>$$<br>q_\pi (s, a) &#x3D; \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a] &#x3D; \sum_{r\in \mathcal{R}}p(r|s, a)r+ \sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s, a)<br>$$</p><p>For model-free model, the action value is the expected return when starting from $(s, a)$. </p><p>$$<br>q_\pi (s, a) &#x3D; \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a] &#x3D; \sum_{r\in \mathcal{R}}p(r|s, a)r+ \sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s, a)<br>$$</p><p>$$<br>q_\pi (s, a) &#x3D; \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a]&#x3D;\mathbb{E}[R_{t+1}+ \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots |S_t&#x3D;s, A_t&#x3D;a]<br>$$</p><p>Starting from $(s, a)$, the agent can interact with the environment by <strong>following policy $\pi_k$ and obtain certain number of episodes.</strong> Suppose there are $n$ episodes and the return for each episode is $g_{\pi_k}^{(i)}(s, a)$, then the $q_\pi (s, a)$ can be expressed as</p><p>$$<br>q_{\pi_k} (s, a) &#x3D; \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a] &#x3D; \frac{1}{n}\sum_{i&#x3D;1}^n g_{\pi_k}^{(i)}(s, a)<br>$$</p><p><strong>Algorithm</strong> </p><p>For the kth iteration, do</p><p>For every state $s \in \mathcal{S}$ in state space, do</p><p>For every action $a \in \mathcal{A}$ in action space, do</p><p>Collecting sufficiently many episodes starting from $(s, a)$ by following $\pi_k$</p><p><strong>Policy Evaluation</strong>:</p><p>$q_{\pi_k} (s,a) \approx q_k(s, a) &#x3D;$  the average return of all the episodes starting from $(s, a)$</p><p><strong>Policy improvement</strong>:</p><p>$a_k^*(s) &#x3D; arg \max_a q_k(s, a)$</p><p>where we can also define the <em><strong>episode length</strong></em>: The length of episode will decide whether an agent can <strong>reach the target</strong> or <strong>get positive reward</strong>.  which also means that if the episode is too small, the return can not be positive so is the <strong>expected state value</strong>. </p><p>The episode length will greatly impact the final policies.  </p><h3 id="sparse-reward"><a href="#sparse-reward" class="headerlink" title="sparse reward"></a>sparse reward</h3><p>which refers to the scenario in which no positive rewards can be obtained unless the target is reached. Therefore, when the state space is too large, it will downgrade the computational efficiency. To solve this problem, we can apply <em><strong>non-sparse reward,</strong></em> which is the setting with “attractive field”, the field except for target which also has positive reward. </p><h2 id="MC-Exploring-Start"><a href="#MC-Exploring-Start" class="headerlink" title="MC Exploring Start"></a>MC Exploring Start</h2><h3 id="Initial-Visit"><a href="#Initial-Visit" class="headerlink" title="Initial Visit"></a>Initial Visit</h3><p>An episode is only used to estimate the <em><strong>action value</strong></em> of the <em><strong>initial state-action pair</strong></em> that the episode starts from.  This is the strategy of “visit” used by <em><strong>MC Basic</strong></em>. </p><p>e.g. Suppose there is an episode  $s1 \xrightarrow{a_2} s3 \xrightarrow{a_1} \cdots$</p><p>This episode only used to estimate $q_t(s1, a2)$, this is sample wasting. </p><p>A state-action pair can be visited multiple times in an episode. </p><h3 id="first-visit"><a href="#first-visit" class="headerlink" title="first-visit"></a>first-visit</h3><p>If only counting the first-time visits. The <strong>samples are approximately independent,</strong> the variance will be smaller in this case. However, the <strong>trajectory of every</strong> $(s, a)$ only has one sample. </p><h3 id="every-visit"><a href="#every-visit" class="headerlink" title="every-visit"></a>every-visit</h3><p>If <strong>counting every visit of a state-action pair.</strong> This is the best for sample usage efficiency. The number of sample is larger, which result in a faster convergence speed. But the sample are highly correlated to each other. </p><p>There are two strategies to <strong>update the policies</strong>.:</p><ul><li>In the policy evaluation step, collecting all the episodes <strong>starting from</strong> the <strong>same state-action pair</strong> and using the <strong>average return of these episode</strong>.  ****</li><li>Use the return of s<strong>ingle episode</strong> to approximate the <strong>single action value</strong>.</li></ul><p><strong>Algorithm 5.2: MC Exploring Starts</strong></p><p>Initialization: Initial Policy $\pi_0(a|s)$  and initial value $q(s, a)$ for all $(s, a)$.  $\text{Returns}(s, a)&#x3D;0$ and $\text{Nums}(s, a)&#x3D;0$.  </p><p>For each episode, do</p><p>Episode generation: Selecting a starting state-action pair $(s_0, a_0)$ and ensure that all pairs can be selected. Following the current policy, generate an episode of length T. </p><p>initialization for each episode: g ← 0</p><p>For each episode, $t &#x3D; T-1, T-2, \cdots, 0$, do</p><p>$g \leftarrow \gamma g + r_{t+1}$</p><p>Returns $(s_t, a_t) \leftarrow$  Returns $(s_t, a_t) + g$</p><p>Nums $(s_t, a_t) \leftarrow$  Nums $(s_t, a_t) + 1$</p><p>Policy Evaluation: </p><p>$q(s_t, a_t) \leftarrow \text{Returns} (s_t, a_t) &#x2F; \text{Nums} (s_t, a_t)$</p><p>Policy Improvement:</p><p>$a_k^*(s) &#x3D; arg \max_a q_k(s, a)$</p><h2 id="MC-epsilon-Greedy"><a href="#MC-epsilon-Greedy" class="headerlink" title="MC $\epsilon$-Greedy"></a>MC $\epsilon$-Greedy</h2><blockquote><p>The fundamental idea of this Greedy Algorithm is to <strong>enhance exploration</strong> by sacrificing optimality&#x2F;exploitation.</p></blockquote><h3 id="soft-policy"><a href="#soft-policy" class="headerlink" title="soft policy"></a>soft policy</h3><p>It has the <strong>positive probability</strong> to <strong>take any action at any state</strong>. With <em><strong>soft policy</strong></em>, a single episode that is <em><strong>sufficiently long</strong></em> can <em><strong>visit every state-action pair many times</strong></em>. </p><p><strong>The policy description</strong></p><p>The MC $\epsilon$-Greedy policy is <strong>a stochastic policy</strong> that has a <strong>higher chance of choosing the greedy action</strong> and the s<strong>ame nonzero probability of taking any other action</strong>. </p>$$\pi(a \mid s) =\begin{cases}1 - \dfrac{\epsilon}{|\mathcal{A}(s)|}\bigl(|\mathcal{A}(s)| - 1\bigr), & \text{for the greedy action}, \\[4pt]\dfrac{\epsilon}{|\mathcal{A}(s)|}, & \text{for other } |\mathcal{A}(s)| - 1 \text{ actions}.\end{cases}$$<p>When the $\epsilon$  is 0, this is totally greedy; when $\epsilon$  is 1, the probability of taking any action equals to $\frac{\epsilon}{|\mathcal{A(s)}|-1}$.</p><p>The <em><strong>Policy Improvement</strong></em> based on $\epsilon$-greedy. </p><p>$$<br>\pi_{k+1}(s) &#x3D; arg \max_{\pi \in \Pi_\epsilon} \sum_a \pi(a|s) q_{\pi_k}(s, a)<br>$$</p><p>The policy is </p>$$\pi(a \mid s) =\begin{cases}1 - \dfrac{\epsilon}{|\mathcal{A}(s)|}\bigl(|\mathcal{A}(s)|-1\bigr), & a = a^*, \\\dfrac{\epsilon}{|\mathcal{A}(s)|}, & a \neq a^*\end{cases}$$<p><strong>Algorithm 5.2: MC Exploring Starts</strong></p><p>Initialization: Initial Policy $\pi_0(a|s)$  and initial value $q(s, a)$ for all $(s, a)$.  $\text{Returns}(s, a)&#x3D;0$ and $\text{Nums}(s, a)&#x3D;0$.  </p><p>For each episode, do</p><p>Episode generation: Selecting a starting state-action pair $(s_0, a_0)$. Following the current policy, generate an episode of length T. </p><p>initialization for each episode: g ← 0</p><p>For each episode, $t &#x3D; T-1, T-2, \cdots, 0$, do</p><p>$g \leftarrow \gamma g + r_{t+1}$</p><p>Returns $(s_t, a_t) \leftarrow$  Returns $(s_t, a_t) + g$</p><p>Nums $(s_t, a_t) \leftarrow$  Nums $(s_t, a_t) + 1$</p><p>Policy Evaluation: </p><p>$q(s_t, a_t) \leftarrow \text{Returns} (s_t, a_t) &#x2F; \text{Nums} (s_t, a_t)$</p><p>Policy Improvement:</p><p>$a^* &#x3D; arg \max_a q(s_t, a)$</p><p>The policy is </p>$$\pi(a \mid s) =\begin{cases}1 - \dfrac{\epsilon}{|\mathcal{A}(s)|}\bigl(|\mathcal{A}(s)| - 1\bigr), & a = a^*, \\[4pt]\dfrac{\epsilon}{|\mathcal{A}(s)|}, & a \neq a^*\end{cases}$$]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Monte Carlo </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning Notes (2)</title>
      <link href="/2025/11/09/RL-note2/"/>
      <url>/2025/11/09/RL-note2/</url>
      
        <content type="html"><![CDATA[<h1 id="Value-Iteration-Policy-Iteration-Truncated-Iteration"><a href="#Value-Iteration-Policy-Iteration-Truncated-Iteration" class="headerlink" title="Value Iteration, Policy Iteration &amp; Truncated Iteration"></a>Value Iteration, Policy Iteration &amp; Truncated Iteration</h1><blockquote><p>The idea of interaction between value and policy update (<em>generalized policy iteration</em>) widely exists in <strong>Reinforcement Learning.</strong> This is also <strong>Dynamic Programming</strong>. These algorithm is also <strong>Model Required</strong>.</p></blockquote><h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><p>Solving the Bellman Optimality Equation through using contraction mapping theorem </p><p>$$<br>v_{k+1} &#x3D; \max_{\pi \in \Pi}(r_\pi + \gamma P_\pi v_k), \quad k &#x3D; 0, 1, 2, \dots<br>$$</p><p>This will guarantee the convergence of optimal state value and policy. </p><p><strong>Step 1</strong>: Policy Update</p><p>$$<br>\pi_{k+1} &#x3D; arg \max_\pi (r_\pi + \gamma P_\pi v_\pi)<br>$$</p><p><strong>Step2</strong>: Value Update</p><p>$$<br>v_{k+1}&#x3D;r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}}v_k<br>$$</p><p>Elementwise Form Analysis</p><p>First, find the best policy: </p><p>$$<br>\pi_{k+1}(s)&#x3D; arg\max_\pi\sum_{a\in \mathcal{A}}\pi(a|s)[\sum_{r\in \mathcal{R}}p(r|s, a)r+ \gamma \sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s, a)], \quad s\in \mathcal{S}<br>$$</p><p>Second, update the state value:</p><p>$$<br>v_{k+1} &#x3D; \sum_{a\in \mathcal{A}}\pi(a|s)[\sum_{r\in \mathcal{R}}p(r|s, a)r+ \gamma\sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s, a)], \quad s\in \mathcal{S}<br>$$</p><p>$a^* &#x3D; arg \max_a q_k(s, a)$, We have </p><p>$$<br>\pi(a|s) &#x3D; \begin{cases} 1, \quad a &#x3D; a^<em>\0, \quad a \neq a^</em>\end{cases}<br>$$</p><p>Thus, the optimal state value after updating is </p><p>$$<br>v_{k+1}(s) &#x3D; \max_a q_k(s, a)<br>$$</p><p>where the $v_k, v_{k+1}$ are only the intermediate value generated in the process of algorithm. </p><p><strong>Algorithm</strong> </p><p>For every state $s \in \mathcal{S}$ in state space</p><p>For every action $a \in \mathcal{A}$ in action space</p><p>First calculate the action value (q-value) by using </p><p>$q_{k}(s, a) &#x3D; \sum_{r\in \mathcal{R}}p(r|s, a)r+ \gamma \sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s, a)$</p><p>choose the maximum action value: $a^*_k(s)&#x3D; arg\max_a q_k(s, a)$ </p><p>update the policy through using $\pi(a|s)$ defined above </p><p>then update the value through $v_{k+1}(s)&#x3D; \max_a q_k(s, a)$</p><h2 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h2><blockquote><p>Indirectly solving the Bellman Equation. The foundation of <em><strong>Monte Carlo (MC)</strong></em>.</p></blockquote><p><strong>Step1</strong>: Policy Evaluation. </p><p>This step will calculate the state value and evaluate the policy. </p><p>$$<br>v_{\pi_k} &#x3D; r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}<br>$$</p><p>where the $r_{\pi_k}, v_{\pi_k}$ can be obtained through system models. </p><p><strong>Step2:</strong> Policy Improvement.</p><p>once $v_<br>{\pi_k}$ is calculated at this first step, we can update the policy through (here is updating the model).</p><p>$$<br>\pi_{k+1} &#x3D; arg\max_\pi(r_\pi + \gamma P_\pi v_{\pi_k})<br>$$</p><p>How to obtain $v_{\pi_k}$</p><p>To calculate $v_{\pi_k}$, we need to do an <strong>embedded iteration</strong> to find the it. </p><p>$$<br>v_{\pi_k}^{(j+1)} &#x3D; r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}^{(j)}, \quad j &#x3D; 0, 1, 2, \cdots<br>$$</p><p>Mathematically, there will be infinite steps to obtain the convergence. However, we can artificially set a threshold for $||v_{\pi_k}^{(j+1)} - v_{\pi_k}^{(j)}||$ or the maximum steps for $j$. </p><p><strong>Why is $\pi_{k+1}$  better than $\pi_k$</strong>?</p><p>$$<br>\because \pi_{k+1} &#x3D; arg\max_\pi(r_\pi + \gamma P_\pi v_{\pi_k})<br>$$</p><p>$$<br>v_{\pi_{k+1}}(s) \geq v_{\pi_k}(s) , \quad \text{for all}\quad s<br>$$</p><blockquote><p>Since the policy iteration contains an embedded iterations, its convergence will be faster than value iteration.</p></blockquote><p>Elementwise form and Algorithm</p><p>While $v_{\pi_k}$ has not converged, for the $k$ th iteration,  do</p><p><strong>Policy Evaluation</strong>:</p><p>Initialization: an arbitrary initial guess $v_{\pi_k}^{(0)}$</p><p>While $v_{\pi_k}^{(j)}$ has not converged, for the $j$ th iteration, do </p><p>For every state $v_{\pi_k}^{(j+1)}(s) &#x3D; \sum_a \pi_k(a|s) [\sum_r p(r|s, a)r + \gamma\sum_{s’}p(s’|s, a)v_{\pi_k}^{(j)}(s’)]$ </p><p><strong>Policy Improvement</strong>:</p><p>For every state $s \in \mathcal{S}$, do:</p><p>For every action $a \in \mathcal A$, do:</p><p>Calculate the action value: $q_{\pi_k}(s, a) &#x3D; \sum_r p(r|s, a)r + \gamma\sum_{s’}p(s’|s, a)v_{\pi_k}(s’)$</p><p>$a_k^*(s) &#x3D; arg\max_a q_{\pi_k} (s, a)$</p><p>$\pi_{k+1}(a|s) &#x3D; 1$  if $a &#x3D; a^*<em>k$ and $\pi</em>{k+1}(a|s)&#x3D;0$ otherwise. </p><p>In the numerical experiment, </p><ul><li>we can find that the states that are close to the target area can find the optimal policy <em><strong>faster than those far away</strong></em>.</li><li>we can also find that the <em><strong>states that are closed to the target area</strong></em> have larger <em><strong>state values</strong></em>.</li></ul><h2 id="Truncated-Policy-Iteration"><a href="#Truncated-Policy-Iteration" class="headerlink" title="Truncated Policy Iteration"></a>Truncated Policy Iteration</h2><p>It is the unified version of Value Iteration and Policy Iteration. </p><p>​<img src="/2025/11/09/RL-note2/figure3.png" class=""></p><p>​<img src="/2025/11/09/RL-note2/figure4.png" class=""></p><ul><li>If the embedded iteration is only <em><strong>one-step</strong></em> instead of <em><strong>infinite steps</strong></em> then the <em><strong>policy iteration</strong></em> will be degraded to <em><strong>value iteration</strong></em>.</li><li>If the embedded iteration is <em><strong>finite steps</strong></em> instead of <em><strong>infinite steps</strong></em> then the <em><strong>policy iteration</strong></em> will be degraded to <em><strong>truncated policy iteration</strong></em>.</li></ul>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Value Iteration </tag>
            
            <tag> Policy Iteration </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Reinforcement Learning Notes (1)</title>
      <link href="/2025/11/09/RL-note1/"/>
      <url>/2025/11/09/RL-note1/</url>
      
        <content type="html"><![CDATA[<h1 id="RL-Basic-Concepts"><a href="#RL-Basic-Concepts" class="headerlink" title="RL - Basic Concepts"></a>RL - Basic Concepts</h1><p>The ultimate goal of reinforcement learning is to obtain the optimal policies.  </p><p>An agent has to interact with the environment to find a optimal policy through trails and errors.</p><h1 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h1><ul><li><strong>Agent</strong>: the object we are observing for finding the best optimal policies.</li><li><strong>States</strong>: $s_i$ The <strong>state space</strong> is $\mathcal{S} &#x3D; {s_1, \cdots s_n}$</li><li><strong>Actions</strong>: $a_i$ The <strong>action space</strong> is $\mathcal{A} &#x3D; {a_1,\cdots, a_m}$</li><li><strong>State Transition</strong>: $s_1 \xrightarrow{a_2} s_2$</li><li><strong>Rewards</strong>: the feedback from the environment. <strong>Positive reward</strong> will encourage an agent to take the corresponding action, and vice versa for <strong>negative reward</strong>. (Human-Machine Interface)</li><li><strong>Returns</strong>: the <strong>return of trajectory</strong> is the <strong>sum of rewards</strong> collected along the trajectory. (Also known as <em>Total rewards or cumulative reward</em>). Returns consists <em><strong>immediate reward</strong></em> and <em><strong>future reward</strong></em>.</li><li><strong>Discounted Returns</strong>: A discounted factor is applied for infinitely long trajectories. It can also <strong>adjust the emphasis on near- or far-future rewards</strong>.</li><li><strong>Policies</strong>: Policy (<strong>stochastic</strong> in general, but sometimes <strong>deterministic</strong>) tells the agent which actions to take at every state, which can be denoted as arrow.</li><li><strong>Trajectory</strong>: a state-action-reward chain. $s_1 \xrightarrow[r&#x3D;0]{a_2} s_2   \xrightarrow[r&#x3D;0]{a_3} s_5   \xrightarrow[r&#x3D;0]{a_3} s_8   \xrightarrow[r&#x3D;1]{a_2} s_9 .$ There is <em><strong>infinite-length(horizon) trajectory</strong></em> except for <em><strong>finite trajectory</strong></em>. So for <em><strong>finite returns</strong></em> and <em><strong>infinite reward.</strong></em></li><li><strong>Immediate Rewards</strong>: reward obtained after taking an action at the initial state.</li><li><strong>Future Rewards</strong>: reward obtained after leaving the initial state.</li><li><strong>Episodes</strong>: a finite trajectory with terminal states whose result trajectory is an episode.</li><li><strong>episodic task&#x2F;continuing task</strong>: whether terminal states exist. The episodic tasks can be mathematically converted to continuing tasks by defining the processes after reaching terminal states.</li><li><strong>visit</strong>: every time a state-action pair appears in an episode, it is called a <em><strong>visit</strong></em> of <strong>state-action pair</strong>.</li><li><strong>Behavior Policy</strong>: it is used to generate experience samples.</li><li><strong>Target Policy</strong>: it is constantly updated to converge to an optimal policy.</li><li><strong>On-policy</strong>: When the <strong>behavior policy</strong> is equal to the <strong>target policy</strong>.</li><li><strong>Off-policy</strong>: When the <strong>behavior policy</strong> is different from the <strong>target policy</strong>. off-policy learning can learn optimal policies based on the experience samples generated by other policies.</li><li><strong>Experience replay:</strong> *The draw of samples is called <em><em>experience replay, which should follow a uniform distribution.</em> The main function of experience replay is to break the correlation of a sequence of action-state pairs. A</em>*fter we have collected some experience samples, we do not use these samples in the order they were collected. They are stored at the <strong>replay buffer</strong>. $(s, a, r, s’)$ is the experience sample and $\mathcal{B}&#x3D; {(s, a, r, s’)}$ is the replay buffer. every time updating the network we will draw a mini-batch from the <strong>replay buffer</strong>.</li></ul><p>Two ways to express <strong>State Transition</strong>:</p><ol><li><p>Table (<strong>only</strong> <strong>deterministic state transition</strong>)</p><img src="/2025/11/09/RL-note1/figure1.jpg" class="" title="State transition table"></li><li><p>Probability Expression</p><img src="/2025/11/09/RL-note1/figure2.jpg" class="" title="State transition probability"></li></ol><h3 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h3><p>Since the state value can be used to evaluate a policy, solving the state values from the Bellman Equation is <em><strong>policy evaluation</strong></em>.</p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Reinforcement Learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
