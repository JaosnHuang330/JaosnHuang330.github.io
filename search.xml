<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Reinforcement Learning Notes</title>
      <link href="/2025/11/08/RL1/"/>
      <url>/2025/11/08/RL1/</url>
      
        <content type="html"><![CDATA[<h1 id="RL-Basic-Concepts"><a href="#RL-Basic-Concepts" class="headerlink" title="RL - Basic Concepts"></a>RL - Basic Concepts</h1><p>The ultimate goal of reinforcement learning is to obtain the optimal policies.  </p><p>An agent has to interact with the environment to find a optimal policy through trails and errors.</p><h1 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h1><ul><li><strong>Agent</strong>: the object we are observing for finding the best optimal policies.</li><li><strong>States</strong>: $s_i$ The <strong>state space</strong> is $\mathcal{S} &#x3D; {s_1, \cdots s_n}$</li><li><strong>Actions</strong>: $a_i$ The <strong>action space</strong> is $\mathcal{A} &#x3D; {a_1,\cdots, a_m}$</li><li><strong>State Transition</strong>: $s_1 \xrightarrow{a_2} s_2$</li><li><strong>Rewards</strong>: the feedback from the environment. <strong>Positive reward</strong> will encourage an agent to take the corresponding action, and vice versa for <strong>negative reward</strong>. (Human-Machine Interface)</li><li><strong>Returns</strong>: the <strong>return of trajectory</strong> is the <strong>sum of rewards</strong> collected along the trajectory. (Also known as <em>Total rewards or cumulative reward</em>). Returns consists <em><strong>immediate reward</strong></em> and <em><strong>future reward</strong></em>.</li><li><strong>Discounted Returns</strong>: A discounted factor is applied for infinitely long trajectories. It can also <strong>adjust the emphasis on near- or far-future rewards</strong>.</li><li><strong>Policies</strong>: Policy (<strong>stochastic</strong> in general, but sometimes <strong>deterministic</strong>) tells the agent which actions to take at every state, which can be denoted as arrow.</li><li><strong>Trajectory</strong>: a state-action-reward chain. $s_1 \xrightarrow[r&#x3D;0]{a_2} s_2Â  Â \xrightarrow[r&#x3D;0]{a_3} s_5Â  Â \xrightarrow[r&#x3D;0]{a_3} s_8Â  Â \xrightarrow[r&#x3D;1]{a_2} s_9 .$ There is <em><strong>infinite-length(horizon) trajectory</strong></em> except for <em><strong>finite trajectory</strong></em>. So for <em><strong>finite returns</strong></em> and <em><strong>infinite reward.</strong></em></li><li><strong>Immediate Rewards</strong>: reward obtained after taking an action at the initial state.</li><li><strong>Future Rewards</strong>: reward obtained after leaving the initial state.</li><li><strong>Episodes</strong>: a finite trajectory with terminal states whose result trajectory is an episode.</li><li><strong>episodic task&#x2F;continuing task</strong>: whether terminal states exist. The episodic tasks can be mathematically converted to continuing tasks by defining the processes after reaching terminal states.</li><li><strong>visit</strong>: every time a state-action pair appears in an episode, it is called a <em><strong>visit</strong></em> of <strong>state-action pair</strong>.</li><li><strong>Behavior Policy</strong>: it is used to generate experience samples.</li><li><strong>Target Policy</strong>: it is constantly updated to converge to an optimal policy.</li><li><strong>On-policy</strong>: When the <strong>behavior policy</strong> is equal to the <strong>target policy</strong>.</li><li><strong>Off-policy</strong>: When the <strong>behavior policy</strong> is different from the <strong>target policy</strong>. off-policy learning can learn optimal policies based on the experience samples generated by other policies.</li><li><strong>Experience replay:</strong> *The draw of samples is called <em><em>experience replay, which should follow a uniform distribution.</em> The main function of experience replay is to break the correlation of a sequence of action-state pairs. A</em>*fter we have collected some experience samples, we do not use these samples in the order they were collected. They are stored at the <strong>replay buffer</strong>. $(s, a, r, sâ€™)$ is the experience sample and $\mathcal{B}&#x3D; {(s, a, r, sâ€™)}$ is the replay buffer. every time updating the network we will draw a mini-batch from the <strong>replay buffer</strong>.</li></ul><p>Two ways to express <strong>State Transition</strong>:</p><ol><li>Table (<strong>only</strong> <strong>deterministic state transition</strong>)</li></ol><p><img src="/RL1/figure1.png" alt="figure1.png"></p><ol><li><p>Probability Expression</p><p> <img src="/RL1/figure2.png" alt="figure2.png"></p></li></ol><h3 id="Policy-Evaluation"><a href="#Policy-Evaluation" class="headerlink" title="Policy Evaluation"></a>Policy Evaluation</h3><p>Since the state value can be used to evaluate a policy, solving the state values from the Bellman Equation is <em><strong>policy evaluation</strong></em>.</p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Welcome to Jason&#39;s CFD World</title>
      <link href="/2025/11/08/cfd-world/"/>
      <url>/2025/11/08/cfd-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <strong>Jasonâ€™s CFD World</strong> ðŸ‘‹</p><p>This blog is my personal notebook as a first-year graduate student exploring:</p><ul><li>Fluid mechanics</li><li>Computational fluid dynamics (CFD)</li><li>Deep learning for scientific computing</li><li>Reinforcement learning for control and optimization</li></ul><p>The goals here are simple:</p><ol><li><strong>Organize my own learning</strong> â€“ short notes, derivations, code snippets, and experiment logs.</li><li><strong>Make things a bit clearer</strong> â€“ write explanations that my past self would understand.</li><li><strong>Track progress</strong> â€“ from basic concepts to more advanced topics over time.</li></ol><p>You can expect posts like:</p><ul><li>Summaries of fundamental fluid mechanics and numerical methods</li><li>Small CFD experiments and implementation notes</li><li>First attempts at using neural networks or RL with flow problems</li><li>Occasional mistakes and fixes (also part of the learning process)</li></ul><p>This is <strong>not</strong> a professional tutorial site or an official guide.<br>Itâ€™s a working notebook in public. If anything here helps you, or if you spot an error, feel free to reach out or compare with better references.</p><p>Thanks for visiting, and welcome to this small corner of CFD &amp; learning.</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
