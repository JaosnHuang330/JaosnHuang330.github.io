<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Reinforcement Learning Notes (1)</title>
      <link href="/2025/11/08/reinforcement-learning-notes(1)/"/>
      <url>/2025/11/08/reinforcement-learning-notes(1)/</url>
      
        <content type="html"><![CDATA[<h1>RL - Basic Concepts</h1><p><strong>The ultimate goal of reinforcement learning is to obtain the optimal policy.</strong></p><p>An agent interacts with the environment and improves its policy through trial and error.</p><h2>Basic Concepts</h2><ul>  <li><strong>Agent</strong>: the decision maker whose behavior we want to optimize.</li>  <li><strong>States</strong>: typically denoted by <code>s_i</code>.      The <strong>state space</strong> is <code>S = {s_1, ..., s_n}</code>.  </li>  <li><strong>Actions</strong>: typically denoted by <code>a_i</code>.      The <strong>action space</strong> is <code>A = {a_1, ..., a_m}</code>.  </li>  <li><strong>State Transition</strong>: how the environment moves from one state to another      given an action, e.g. <code>s_1 --(a_2)--> s_2</code>.  </li>  <li><strong>Rewards</strong>: feedback from the environment.      Positive rewards encourage an action; negative rewards discourage it.  </li>  <li><strong>Returns</strong>: the sum of rewards along a trajectory      (also called total / cumulative reward), including immediate and future rewards.  </li>  <li><strong>Discounted Returns</strong>: use a discount factor to handle infinite-horizon      tasks and to control the importance of near vs. far future rewards.  </li>  <li><strong>Policy</strong>: a (possibly stochastic) mapping from states to actions.      It tells the agent what to do in each state.  </li>  <li><strong>Trajectory</strong>: a sequence of      <code>(state, action, reward, next state)</code>, e.g.      <code>sâ‚ --(aâ‚‚, r=0)--> sâ‚‚ --(aâ‚ƒ, r=0)--> sâ‚… --(aâ‚ƒ, r=0)--> sâ‚ˆ --(aâ‚‚, r=1)--> sâ‚‰</code>.  </li>  <li><strong>Immediate Reward</strong>: reward received right after taking an action at the current state.</li>  <li><strong>Future Rewards</strong>: rewards received after leaving the current state.</li>  <li><strong>Episode</strong>: a finite trajectory that ends in a terminal state.</li>  <li><strong>Episodic vs. Continuing Tasks</strong>:      episodic tasks have terminal states; continuing tasks do not.      Episodic tasks can sometimes be reformulated as continuing ones.  </li>  <li><strong>Visit</strong>: each occurrence of a particular state-action pair in a trajectory.</li>  <li><strong>Behavior Policy</strong>: the policy used to generate experience samples.</li>  <li><strong>Target Policy</strong>: the policy we are trying to improve towards optimality.</li>  <li><strong>On-policy</strong>: behavior policy = target policy.</li>  <li><strong>Off-policy</strong>: behavior policy â‰  target policy.      We learn about one policy from data generated by another.  </li>  <li><strong>Experience Replay</strong>:      store transitions <code>(s, a, r, s')</code> in a replay buffer      <code>B = {(s, a, r, s')}</code> and sample them (often uniformly) when updating.      This helps break the temporal correlation between samples.  </li></ul><h2>State Transition Representations</h2><ol>  <li>    <strong>Table form</strong> (for small / deterministic environments)    <figure>      <img src="RL-Basic-Concepts/Screenshot_2025-11-02_at_15.40.26.png"           alt="Example of state transition table">    </figure>  </li>  <li>    <strong>Probability expression</strong>:    transition dynamics written as    <code>P(s' | s, a)</code>.    <figure>      <img src="RL-Basic-Concepts/Screenshot_2025-11-02_at_15.45.00.png"           alt="Example of probabilistic state transition notation">    </figure>  </li></ol><h3>Policy Evaluation</h3><p>  The state value function under a given policy can be defined using the Bellman equation.  Solving for these state values is called <strong>policy evaluation</strong>.</p>]]></content>
      
      
      <categories>
          
          <category> Reinforcement Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Reinforcement Learning </tag>
            
            <tag> Notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Welcome to Jason&#39;s CFD World</title>
      <link href="/2025/11/08/cfd-world/"/>
      <url>/2025/11/08/cfd-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <strong>Jasonâ€™s CFD World</strong> ðŸ‘‹</p><p>This blog is my personal notebook as a first-year graduate student exploring:</p><ul><li>Fluid mechanics</li><li>Computational fluid dynamics (CFD)</li><li>Deep learning for scientific computing</li><li>Reinforcement learning for control and optimization</li></ul><p>The goals here are simple:</p><ol><li><strong>Organize my own learning</strong> â€“ short notes, derivations, code snippets, and experiment logs.</li><li><strong>Make things a bit clearer</strong> â€“ write explanations that my past self would understand.</li><li><strong>Track progress</strong> â€“ from basic concepts to more advanced topics over time.</li></ol><p>You can expect posts like:</p><ul><li>Summaries of fundamental fluid mechanics and numerical methods</li><li>Small CFD experiments and implementation notes</li><li>First attempts at using neural networks or RL with flow problems</li><li>Occasional mistakes and fixes (also part of the learning process)</li></ul><p>This is <strong>not</strong> a professional tutorial site or an official guide.<br>Itâ€™s a working notebook in public. If anything here helps you, or if you spot an error, feel free to reach out or compare with better references.</p><p>Thanks for visiting, and welcome to this small corner of CFD &amp; learning.</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
