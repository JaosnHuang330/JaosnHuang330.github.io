<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 8.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo-pixel-flow-advanced.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"always","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"manual","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="This note will explain baisc concepts of RL and Bellman Equation.">
<meta property="og:type" content="article">
<meta property="og:title" content="RL Note 1: Basic Concepts &amp; Tools for RL">
<meta property="og:url" content="http://example.com/2025/11/09/RL-note1/index.html">
<meta property="og:site_name" content="Jason&#39;s CFD World">
<meta property="og:description" content="This note will explain baisc concepts of RL and Bellman Equation.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/2025/11/09/RL-note1/figure1.png">
<meta property="og:image" content="http://example.com/2025/11/09/RL-note1/figure2.png">
<meta property="og:image" content="http://example.com/2025/11/09/RL-note1/figure3.png">
<meta property="og:image" content="http://example.com/2025/11/09/RL-note1/figure5.png">
<meta property="og:image" content="http://example.com/2025/11/09/RL-note1/figure6.png">
<meta property="og:image" content="http://example.com/2025/11/09/RL-note1/figure7.png">
<meta property="og:image" content="http://example.com/2025/11/09/RL-note1/figure8.png">
<meta property="og:image" content="http://example.com/2025/11/09/RL-note1/figure8.png">
<meta property="og:image" content="http://example.com/2025/11/09/RL-note1/figure9.png">
<meta property="article:published_time" content="2025-11-09T04:11:01.000Z">
<meta property="article:modified_time" content="2025-11-11T13:31:26.642Z">
<meta property="article:author" content="Jason Huang">
<meta property="article:tag" content="Bellman Equation">
<meta property="article:tag" content="Reinforcement Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2025/11/09/RL-note1/figure1.png">

<link rel="canonical" href="http://example.com/2025/11/09/RL-note1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>RL Note 1: Basic Concepts & Tools for RL | Jason's CFD World</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/rss2.xml" title="Jason's CFD World" type="application/rss+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Jason's CFD World</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">9</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">2</span></a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">20</span></a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-book fa-fw"></i>Resources</a>

  </li>
        <li class="menu-item menu-item-schedule">

    <a href="/schedule/" rel="section"><i class="fa fa-calendar fa-fw"></i>Schedule</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/JaosnHuang330" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/11/09/RL-note1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/jason_avatar.jpg">
      <meta itemprop="name" content="Jason Huang">
      <meta itemprop="description" content="Explore the Beauty of Flows">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Jason's CFD World">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          RL Note 1: Basic Concepts & Tools for RL
        </h1>

        <div class="post-meta">
	  
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2025-11-09 13:11:01" itemprop="dateCreated datePublished" datetime="2025-11-09T13:11:01+09:00">2025-11-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2025-11-11 22:31:26" itemprop="dateModified" datetime="2025-11-11T22:31:26+09:00">2025-11-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Reinforcement-Learning/" itemprop="url" rel="index"><span itemprop="name">Reinforcement Learning</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

<span class="index-cat-tag">
  
    <span class="post-meta-item">
      <i class="fa fa-tag"></i>
      
        <a class="post-tag-link" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a>, 
      
        <a class="post-tag-link" href="/tags/Bellman-Equation/">Bellman Equation</a>
      
    </span>
  
</span>
<br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>14k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>12 mins.</span>
            </span>
            <div class="post-description">This note will explain baisc concepts of RL and Bellman Equation.</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Key-Terms"><a href="#Key-Terms" class="headerlink" title="Key Terms"></a>Key Terms</h1><ul>
<li><strong>Agent</strong>: the object we are observing for finding the best optimal policies.</li>
<li><strong>States</strong>: $s_i$ The <strong>state space</strong> is $\mathcal{S} &#x3D; {s_1, \cdots s_n}$</li>
<li><strong>Actions</strong>: $a_i$ The <strong>action space</strong> is $\mathcal{A} &#x3D; {a_1,\cdots, a_m}$</li>
<li><strong>State Transition</strong>: $s_1 \xrightarrow{a_2} s_2$</li>
<li><strong>Rewards</strong>: the feedback from the environment. <strong>Positive reward</strong> will encourage an agent to take the corresponding action, and vice versa for <strong>negative reward</strong>. (Human-Machine Interface)</li>
<li><strong>Returns</strong>: the <strong>return of trajectory</strong> is the <strong>sum of rewards</strong> collected along the trajectory. (Also known as <em>Total rewards or cumulative reward</em>). Returns consists <em><strong>immediate reward</strong></em> and <em><strong>future reward</strong></em>.</li>
<li><strong>Discounted Returns</strong>: A discounted factor is applied for infinitely long trajectories. It can also <strong>adjust the emphasis on near- or far-future rewards</strong>.</li>
<li><strong>Policies</strong>: Policy (<strong>stochastic</strong> in general, but sometimes <strong>deterministic</strong>) tells the agent which actions to take at every state, which can be denoted as arrow.</li>
<li><strong>Trajectory</strong>: a state-action-reward chain. $s_1 \xrightarrow[r&#x3D;0]{a_2} s_2   \xrightarrow[r&#x3D;0]{a_3} s_5   \xrightarrow[r&#x3D;0]{a_3} s_8   \xrightarrow[r&#x3D;1]{a_2} s_9 .$ There is <em><strong>infinite-length(horizon) trajectory</strong></em> except for <em><strong>finite trajectory</strong></em>. So for <em><strong>finite returns</strong></em> and <em><strong>infinite reward.</strong></em></li>
<li><strong>Immediate Rewards</strong>: reward obtained after taking an action at the initial state.</li>
<li><strong>Future Rewards</strong>: reward obtained after leaving the initial state.</li>
<li><strong>Episodes</strong>: a finite trajectory with terminal states whose result trajectory is an episode.</li>
<li><strong>episodic task&#x2F;continuing task</strong>: whether terminal states exist. The episodic tasks can be mathematically converted to continuing tasks by defining the processes after reaching terminal states.</li>
<li><strong>visit</strong>: every time a state-action pair appears in an episode, it is called a <em><strong>visit</strong></em> of <strong>state-action pair</strong>.</li>
<li><strong>Behavior Policy</strong>: it is used to generate experience samples.</li>
<li><strong>Target Policy</strong>: it is constantly updated to converge to an optimal policy.</li>
<li><strong>On-policy</strong>: When the <strong>behavior policy</strong> is equal to the <strong>target policy</strong>.</li>
<li><strong>Off-policy</strong>: When the <strong>behavior policy</strong> is different from the <strong>target policy</strong>. off-policy learning can learn optimal policies based on the experience samples generated by other policies.</li>
<li><strong>Experience replay:</strong> <em>The draw of samples is called <strong>experience replay</strong>, which should follow a uniform distribution.</em> The main function of experience replay is to break the correlation of a sequence of action-state pairs. <strong>After</strong> we have collected some experience samples, we do not use these samples in the order they were collected. They are stored at the <strong>replay buffer</strong>. $(s, a, r, s’)$ is the experience sample and $\mathcal{B}&#x3D; {(s, a, r, s’)}$ is the replay buffer. Every time updating the network we will draw a mini-batch from the <strong>replay buffer</strong>.</li>
<li><strong>Policy Evaluation</strong>: <em><strong>Policy Evalution is the process calculating state value or action value based on given policy</strong> when the system dynamics is given or samplable.</em> Since the state value can be used to evaluate a policy, solving the state values from the Bellman Equation is <em><strong>policy evaluation</strong></em>. Basically, the task of Policy Evalution is to solve the Bellman Equation.</li>
</ul>
<p>Two ways to express <strong>Deterministic</strong> <strong>State Transition</strong>:</p>
<ol>
<li>Table (<strong>only</strong> <strong>deterministic state transition</strong>)</li>
</ol>
<p>​	<img src="/2025/11/09/RL-note1/figure1.png" class="" title="figure1">	</p>
<ol>
<li><p>Probability Expression</p>
 <img src="/2025/11/09/RL-note1/figure2.png" class="" title="figure2"></li>
</ol>
<h1 id="Key-Concept-Key-Tool"><a href="#Key-Concept-Key-Tool" class="headerlink" title="Key Concept &amp; Key Tool"></a>Key Concept &amp; Key Tool</h1><h2 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h2><h3 id="State-Value"><a href="#State-Value" class="headerlink" title="State Value"></a>State Value</h3><blockquote>
<p>It’s the <strong>expected return</strong> (<strong>average reward</strong> &#x2F; <strong>total reward</strong>) that an agent can obtain when <strong>starting from a state</strong> through <strong>following a given policy</strong>.  It can be taken as a metric to evaluate the goodness of a policy.</p>
</blockquote>
<p>The returns can be utilized to evaluate policies. However, it is inapplicable to stochastic systems, because starting from one state will lead to different returns. </p>
<p>Therefore,   <strong>returns → state values</strong></p>
<p>$$<br>S_t \xrightarrow{A_t} S_{t+1}, R_{t+1}<br>$$</p>
<p>$S_t, A_t, S_{t+1}, R_{t+1}$ are all random variables.  $A_t \in \mathcal{A}(S_t)$  is the action taken by following Policy $\pi$. The immediate reward is $R_{t+1} \in \mathcal{R}(S_t,A_t)$. Therefore, the <strong>discounted return</strong> along the trajectory is </p>
<p>$$<br>G_t &#x3D; R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}+ \cdots,<br>$$</p>
<p>where the $G_t$ is a <em><strong>random variable</strong></em>, which can be obtained by calculating <em><strong>expected value</strong></em>. Since the policy is given, $A_t$ is not a random variable here. </p>
<p>$$<br>v_\pi(s) &#x3D; \mathbb{E}[G_t|S_t &#x3D; s]<br>$$</p>
<p>This expected value $v_\pi (s)$  is called the <em><strong>state-value function</strong></em> or <em><strong>state value</strong></em> of $s$ .</p>
<h3 id="Motivating-Example"><a href="#Motivating-Example" class="headerlink" title="Motivating Example:"></a>Motivating Example:</h3><p>​	<img src="/2025/11/09/RL-note1/figure3.png" class="" title="figure3"></p>
<p>The return by following the first policy is $\text{return}_{1} &#x3D; 0 + \gamma1 + \gamma 1 + \cdots &#x3D; \frac{\gamma}{1 - \gamma}$;</p>
<p>The return by following the second policy is $\text{return}_2 &#x3D; -1 + \frac{\gamma}{1- \gamma}$;</p>
<p>The thrid return is $\text{return}_3 &#x3D; 0.5 \times \text{return}_1 +  0.5 \times \text{return}_2  &#x3D; -0.5  + \frac{\gamma}{1- \gamma}$</p>
<h3 id="The-idea-to-calculate-returns"><a href="#The-idea-to-calculate-returns" class="headerlink" title="The idea to calculate returns:"></a>The idea to calculate returns:</h3><p>​	</p>
<p>$$<br>v_1 &#x3D; r_1 + \gamma (r_2 + \gamma r_3 + \cdots) &#x3D; r_1 + \gamma v_2<br>$$</p>
<p>$$<br>v_2 &#x3D; r_2 + \gamma (r_3 + \gamma r_4 + \cdots) &#x3D; r_2 + \gamma v_3<br>$$</p>
<p>$$<br>v_3 &#x3D; r_3 + \gamma (r_4 + \gamma r_1 + \cdots) &#x3D; r_3 + \gamma v_4<br>$$</p>
<p>$$<br>v_4 &#x3D; r_4 + \gamma (r_1 + \gamma r_2 + \cdots) &#x3D; r_4 + \gamma v_1<br>$$</p>
<p>The idea is <em><strong>bootstrapping:</strong></em> using current estimates to update futrue estimates .  It is also an endless loop because the calculation of an unknown value relies on another unknown value. </p>
<h3 id="Action-Value"><a href="#Action-Value" class="headerlink" title="Action Value"></a>Action Value</h3><blockquote>
<p><strong>Action value</strong> indicates the value (<em><strong>expected return</strong></em>) of taking an action at a state. The action value is highly dependent on <em><strong>State Value</strong>.</em> It is also known as <em><strong>state-action value</strong></em>. Action value and state value can be converted to each other.</p>
</blockquote>
<p>The action value of a state-action pair can be defined as </p>
<p>$$<br>\begin{aligned}<br>q_\pi(s,a)<br>&amp;&#x3D; \mathbb{E}\big[ G_t \mid S_t&#x3D;s, A_t&#x3D;a \big] \<br>&amp;&#x3D; \sum_{r \in \mathcal{R}} p(r \mid s,a), r</p>
<ul>
<li>\sum_{s’ \in \mathcal{S}} p(s’ \mid s,a), v_\pi(s’)<br>\end{aligned}<br>$$</li>
</ul>
<p><em><strong>This shows how to obtain action values through using state values.</strong></em><br>$$<br>\because \mathbb{E}[G_t|S_t &#x3D; s] &#x3D; \sum_{a\in \mathcal{A}}\pi(a|s)[\sum_{r\in \mathcal{R}}p(r|s, a)r+ \sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s, a)]  \ &#x3D;  \sum_{a\in \mathcal{A}}\mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a]\pi(a|s)<br>$$</p>
<p><strong>This shows how to obtain state values through using action values.</strong><br>$$<br>v_\pi(s) &#x3D; \sum_{a\in \mathcal{A}}\pi(a|s)q_\pi(s,a)<br>$$</p>
<h3 id="Optimal-Policy"><a href="#Optimal-Policy" class="headerlink" title="Optimal Policy"></a>Optimal Policy</h3>

The state values of optimal policy is the greatest compared to other policies. It can also be written as Matrix Form.


$$
v^*(s) = \max_{\pi} v^\pi(s),\quad
v^{\pi^*}(s) \ge v^\pi(s),\ \forall s,\pi.
$$



<h3 id="Optimal-State-Value"><a href="#Optimal-State-Value" class="headerlink" title="Optimal State Value"></a>Optimal State Value</h3>

We can obtain the optimal policies based on optimal state value that solved from Bellman Optimality Equation. A policy $\pi^*$  is optimal if $v_{\pi^*}(s)> v_\pi (s)$ for all $s \in \mathcal{S}$ and for any other policy $\pi$. The state values of $\pi^*$ are the optimal state values. Therefore, an optimal policy has the greatest state value for every state compared to any other policies. The Bellman optimality equation can be written as 
$$
v^*(s) = \max_a[r(s, a),\gamma \sum_{s'}P(s'|s, a)v^*(s')]
$$



<h2 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h2><h3 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h3><blockquote>
<p>Bellman Equation is basically a set of linear equations that describe the relationship between values of all the states. Bellman equation can be used to analyze the state values. It describes the relationship between values of all states.</p>
</blockquote>
<p>The below is the return from decision epoch $t$. </p>
<p>$$<br>G_t &#x3D; R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \cdots ) &#x3D; R_{t+1} + \gamma G_{t+1}<br>$$</p>
<p>Therefore, the state value we determined before can be written as</p>
<p>$$<br>v_{\pi}(s) &#x3D; \mathbb{E}[G_t|S_t &#x3D; s]&#x3D; \mathbb{E}[R_{t+1}+ \gamma G_{t+1}] &#x3D; \mathbb{E}[R_{t+1}|S_{t}&#x3D;s] + \gamma \mathbb{E}[G_{t+1}|S_t &#x3D; s]<br>$$</p>
<p>$$<br>\mathbb{E}[R_{t+1}|S_t &#x3D; s] &#x3D; \sum_{a\in \mathcal{A}}\pi(a|s) \mathbb{E}[R_{t+1}|S_{t+1}&#x3D;s, A_t&#x3D;a] &#x3D; \sum_{a\in \mathcal{A}} \pi(a|s) \sum_{r\in \mathcal{R}}p(r|s, a)r<br>$$</p>
<p>$$<br>\mathbb{E}[G_{t+1}|S_t &#x3D;s]&#x3D; \sum_{s’\in \mathcal{S}}\mathbb{E}[G_{t+1}|S_t &#x3D; s, S_{t+1}&#x3D;s’]p(s’|s)&#x3D; \sum_{s’\in \mathcal{S}}\mathbb{E}[G_{t+1}|  S_{t+1}&#x3D;s’]p(s’|s) &#x3D; \sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s) &#x3D;\sum_{s’\in \mathcal{S}}v_\pi(s’)\sum_{a\in \mathcal{A}}\pi(a|s)p(s’|s, a)<br>$$</p>
<p>The Bellman Equation will be</p>
<p>$$<br>v_\pi(s) &#x3D; \mathbb{E}[G_t|S_t&#x3D;s] &#x3D; \mathbb{E}[R_{t+1}|S_t&#x3D;s] + \gamma \mathbb{E}[G_{t+1}|S_t&#x3D;s] &#x3D; \sum_{a\in \mathcal{A}} \pi(a|s) \sum_{r\in \mathcal{R}}p(r|s, a)r +\sum_{s’\in \mathcal{S}}v_\pi(s’)\sum_{a\in \mathcal{A}}\pi(a|s)p(s’|s, a) &#x3D; \sum_{a\in \mathcal{A}}\pi(a|s)[\sum_{r\in \mathcal{R}}p(r|s, a)r +\sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s, a)]<br>$$</p>
<p>the Bellman Equation is written as the sum of <em><strong>mean of immediate rewards</strong></em> and <em><strong>mean of future rewards</strong></em>. In the equation, the probability models, policies, rewards is known or given. </p>
<p>​	<img src="/2025/11/09/RL-note1/figure5.png" class="" title="figure5"></p>
<p>​	<img src="/2025/11/09/RL-note1/figure6.png" class="" title="figure6"></p>
<p>​	<img src="/2025/11/09/RL-note1/figure7.png" class="" title="figure7"></p>
<p>The above derivations and results are all elementwise form, there are also matrix-vector form. </p>
<h3 id="Matrix-Vector-Form"><a href="#Matrix-Vector-Form" class="headerlink" title="Matrix-Vector Form"></a>Matrix-Vector Form</h3><p>$$<br>v_\pi &#x3D; r_\pi + \gamma P_\pi v_\pi<br>$$</p>
<p>​	<img src="/2025/11/09/RL-note1/figure8.png" class="" title="figure8"></p>
<p>The solution is $v_\pi &#x3D; (I-\gamma P_\pi)^{-1}r_\pi$, where $(I-\gamma P_\pi)$ is invertible, and each element of $(I-\gamma P_\pi)^{-1}$ is no less than the identity matrix. </p>
<p>However, to obtain this solution, we have to calculate the inverse matrix. Instead, we usually apply iterative solution. </p>
<p>$$<br>v_{k+1} &#x3D; r_{\pi} +\gamma P_\pi v_k, \quad k &#x3D; 0, 1, 2, \cdots<br>$$</p>
<p>  where the $v_0$ is the initial guess of $v_\pi$.</p>
<h3 id="Bellman-Equation-in-terms-of-Action-Value"><a href="#Bellman-Equation-in-terms-of-Action-Value" class="headerlink" title="Bellman Equation in terms of Action Value"></a>Bellman Equation in terms of Action Value</h3><p>$$<br> v_k \rightarrow v_\pi &#x3D; (I-\gamma P_\pi)^{-1}r_\pi, \quad \text{as}\quad  k  \rightarrow \infty<br>$$</p>
<p>$$<br>q_\pi(s, a)&#x3D;\sum_{r\in \mathcal{R}}p(r|s, a)r+ \sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s, a)<br>$$</p>
<p>and </p>
<p>$$<br>v_\pi (s) &#x3D; \sum_{a\in \mathcal{A}}\pi(a|s)q_\pi(s, a)<br>$$</p>
<p>then, the Bellman Equation becomes. </p>
<p>$$<br>q_\pi(s, a)&#x3D;\sum_{r\in \mathcal{R}}p(r|s, a)r+ \sum_{s’\in \mathcal{S}}p(s’|s, a)\sum_{a’\in \mathcal{A(s’)}}\pi(a’|s’)q_\pi(s’, a’)<br>$$</p>
<h3 id="Bellman-Optimality-Equation"><a href="#Bellman-Optimality-Equation" class="headerlink" title="Bellman Optimality Equation"></a>Bellman Optimality Equation</h3><p>This equation is used to obtain the optimal policies, that is, we can use Bellman Optimality Equation to solve the <strong>optimal state values and policies</strong>. </p>
<h3 id="How-to-improve-policies"><a href="#How-to-improve-policies" class="headerlink" title="How to improve policies?"></a>How to improve policies?</h3><p>Mathematics: The problem can be realized based on the calculation of <em><strong>state values</strong></em> and <em><strong>action values</strong></em>.  </p>
<p>First, we can calculate the state values based on the given policy, like this: </p>
<p>​	<img src="/2025/11/09/RL-note1/figure8.png" class="" title="figure8"></p>
<p>Second, we can calculate the <strong>action values</strong> for each state. Here we only take state $s_1$ as an example. </p>
<blockquote>
<p>Here we have to emphasize the concept of action value again: it is the expected returns obtain at a given state by conducting a certain action.</p>
</blockquote>
<p>​	<img src="/2025/11/09/RL-note1/figure9.png" class="" title="figure9"></p>
<p>Obviously, $a_3$ is the greatest action. </p>
<p>For every $s \in \mathcal{S}$ , the <strong>elementwise expression</strong> of the BOE (a set of linear equation for updating state value) is </p>
<p>$$<br>v(s) &#x3D; \max_{\pi(s) \in \Pi(s)} \sum_{a\in \mathcal{A}}\pi(a|s) (\sum_{r\in\mathcal{R}}p(r|s, a)r + \gamma \sum_{s’\in \mathcal{S}}p(s’|s, a)v(s’)) &#x3D; \max_{\pi(s) \in \Pi(s)}\sum_{a\in\mathcal{A}}\pi(a|s)q(s, a)<br>$$</p>
<p>$\pi(s)$ is a policy for state $s$, $\Pi(s)$ is the set of all possible policies for $s$</p>
<p>Although the BOE has two unknown variables $v(s)$ and $\pi(a|s)$, they can be solved one by one. </p>
<p>Because of the structure of this Equation, we can be inspired by the below example!</p>
<p>​	</p>
<p>Here we have $\sum_a \pi(a|s) &#x3D; 1$, we have</p>
<p>$$<br>\sum_{a\in \mathcal{A}}\pi(s|a)q(s, a) &#x3D; \sum_{a\in \mathcal{A}}\pi(s|a) \max_{a\in \mathcal{A}} q(s, a) &#x3D; \max_{a\in \mathcal{A}}q(s, a)<br>$$</p>
<p>where equality is achieved when </p>
<h3 id="Greedy-Optimal-Policy"><a href="#Greedy-Optimal-Policy" class="headerlink" title="Greedy Optimal Policy"></a>Greedy Optimal Policy</h3>

this is an optimal policy for solving the BOE
$$
\pi(a|s) = \begin{cases} 1, \quad a = a^*\\0, \quad a \neq a^*\end{cases}
$$

The matrix-vector form of BOE is 

$$
v = \max_{\pi \in \Pi}(r_\pi + \gamma P_\pi v)
$$

it can be expressed as 

$$
f(v) = \max_{\pi \in \Pi}(r_\pi + \gamma P_\pi v)
$$

therefore, we only need to achieve $v = f(v)$, here we have to use Contraction Mapping Theorem (Fixed Pointed Theorem). 

Consider a function $f(x)$, where $x \in \mathbb{R}^d$ and $f: \mathbb{R}^d \rightarrow \mathbb{R}^d$. The point $x^*$ is called fixed point if 

$f(x^*) = x^*$. Therefore the function $f$ is the contraction mapping, if there exists $\gamma \in(0, 1)$ 

$$
||f(x_1)-f(x_2)|| \leq \gamma ||x_1 - x_2||
$$



<p><strong>Theorem 3.1</strong> (Contraction Mapping Theorem). For any equation that has the form $x &#x3D;f(x)$ where $x$ and $f(x)$ are real vectors, if $f$ is a contraction mapping, then the following properties hold. </p>
<ul>
<li>Existence:   There exists a fixed point $x^*$ satisfying  $f(x^*)=x^*$. </li>
<li>Uniqueness: The fixed point $x^*$ is unique.</li>
<li>Algorithm: Consider the iterative process: $x_{k+1} &#x3D; f(x_k)$</li>
</ul>


where $x_k \rightarrow x^*$, $k \rightarrow \infty$ for any initial guess $x_0$.  

The function $f(v)$ on the right-hand side of the BOE in (3.3) is a contraction mapping. For any $v_1, v_2 \in \mathbb{R}^{|S|}$, it holds that

$$
||f(v_1)-f(v_2)||_\infty \leq \gamma ||v_1 -v_2||_\infty
$$

here we use the maximum norm. 



<h3 id="solving-optimal-state-values"><a href="#solving-optimal-state-values" class="headerlink" title="solving optimal state values"></a>solving optimal state values</h3>

Therefore, for the BOE $v = f(v) = \max_{\pi \in \Pi}(r_\pi + \gamma P_\pi v^*)$. 

$v^*$ is the fixed point. There always exists a unique solution $v^*$, which can be solved iteratively by 

$$
v_{k+1} = f(v_k)= \max_{\pi \in \Pi}(r_\pi + \gamma P_\pi v_k)
$$



<h3 id="solving-optimal-policy"><a href="#solving-optimal-policy" class="headerlink" title="solving optimal policy"></a>solving optimal policy</h3>

after we obtain the optimal state values $v^*$, we can easily obtain $\pi^*$ by solving 

$$
\pi^* = arg\max_{\pi\in \Pi} (r_\pi + \gamma P_\pi v^*)
$$



<p>Then </p>

$$
v^* = r_{\pi^*} + \gamma P_{\pi^*}v^*
$$



<h3 id="Greedy-Optimal-Policy-1"><a href="#Greedy-Optimal-Policy-1" class="headerlink" title="Greedy Optimal Policy"></a>Greedy Optimal Policy</h3><p>For any $s \in \mathcal{S}$, the <em><strong>deterministic greedy policy</strong></em></p>

$$
\pi(a|s) = \begin{cases} 1, \quad a = a^*\\0, \quad a \neq a^*\end{cases}
$$





Here, $a^*(s) = arg \max_a q^*(a, s)$ , where




$$
q^*(s, a) = \sum_{r\in \mathcal{R}}p(r|s, a)r + \gamma \sum_{s' \in \mathcal{S}}p(s'|s, a)v^*(s')
$$

    </div>

    
    
    

    
      <div>
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:24px;">-------------Thank you for your reading!-------------</div>
    
</div>


      </div>
    
  <div class="index-tags">
    
      <a class="index-tag" href="/tags/Reinforcement-Learning/">Reinforcement Learning</a>
    
      <a class="index-tag" href="/tags/Bellman-Equation/">Bellman Equation</a>
    
  </div>


        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:  </strong>Jason Huang
  </li>
  <li class="post-copyright-link">
    <strong>Post link: </strong>
    <a href="http://example.com/2025/11/09/RL-note1/" title="RL Note 1: Basic Concepts &amp; Tools for RL">http://example.com/2025/11/09/RL-note1/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice:  </strong>All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> unless stating additionally.
  </li>
</ul>
</div>

        

  <div class="followme">
    <p>Welcome to my other publishing channels</p>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/images/wechat_channel.jpg">
            <span class="icon">
              <i class="fab fa-weixin"></i>
            </span>

            <span class="label">WeChat</span>
          </a>
        </div>

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
              <a href="/tags/Bellman-Equation/" rel="tag"># Bellman Equation</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2025/11/09/RL-note2/" rel="next" title="RL Note 2: Value Iteration & Policy Iteration">
      RL Note 2: Value Iteration & Policy Iteration <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>

  
  
  



          </div>
          
    
  <div class="comments">
    <div id="lv-container" data-id="city" data-uid="MTAyMC82MDk5NC8zNzQ2NA=="></div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Key-Terms"><span class="nav-number">1.</span> <span class="nav-text">Key Terms</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Key-Concept-Key-Tool"><span class="nav-number">2.</span> <span class="nav-text">Key Concept &amp; Key Tool</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Concepts"><span class="nav-number">2.1.</span> <span class="nav-text">Concepts</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#State-Value"><span class="nav-number">2.1.1.</span> <span class="nav-text">State Value</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Motivating-Example"><span class="nav-number">2.1.2.</span> <span class="nav-text">Motivating Example:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-idea-to-calculate-returns"><span class="nav-number">2.1.3.</span> <span class="nav-text">The idea to calculate returns:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Action-Value"><span class="nav-number">2.1.4.</span> <span class="nav-text">Action Value</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimal-Policy"><span class="nav-number">2.1.5.</span> <span class="nav-text">Optimal Policy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Optimal-State-Value"><span class="nav-number">2.1.6.</span> <span class="nav-text">Optimal State Value</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tools"><span class="nav-number">2.2.</span> <span class="nav-text">Tools</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Bellman-Equation"><span class="nav-number">2.2.1.</span> <span class="nav-text">Bellman Equation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Matrix-Vector-Form"><span class="nav-number">2.2.2.</span> <span class="nav-text">Matrix-Vector Form</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bellman-Equation-in-terms-of-Action-Value"><span class="nav-number">2.2.3.</span> <span class="nav-text">Bellman Equation in terms of Action Value</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Bellman-Optimality-Equation"><span class="nav-number">2.2.4.</span> <span class="nav-text">Bellman Optimality Equation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#How-to-improve-policies"><span class="nav-number">2.2.5.</span> <span class="nav-text">How to improve policies?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Greedy-Optimal-Policy"><span class="nav-number">2.2.6.</span> <span class="nav-text">Greedy Optimal Policy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#solving-optimal-state-values"><span class="nav-number">2.2.7.</span> <span class="nav-text">solving optimal state values</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#solving-optimal-policy"><span class="nav-number">2.2.8.</span> <span class="nav-text">solving optimal policy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Greedy-Optimal-Policy-1"><span class="nav-number">2.2.9.</span> <span class="nav-text">Greedy Optimal Policy</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jason Huang"
      src="/images/jason_avatar.jpg">
  <p class="site-author-name" itemprop="name">Jason Huang</p>
  <div class="site-description" itemprop="description">Explore the Beauty of Flows</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">20</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/JaosnHuang330" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;JaosnHuang330" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhiyuanh330@gmail.com" title="E-Mail → mailto:zhiyuanh330@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title"><i class="link fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://fpe.postech.ac.kr/postech" title="https:&#x2F;&#x2F;fpe.postech.ac.kr&#x2F;postech" rel="noopener" target="_blank">Flow Phycis and Engineering</a>
        </li>
    </ul>
  </div>

<script src="https://cdn.jsdelivr.net/npm/jquery@3.7.1/dist/jquery.min.js"></script>


<div class="sidebar-card sidebar-focus">
  <div class="sidebar-card-title">Now learning</div>
  <ul class="sidebar-focus-list">
    <li>Reinforcement Learning</li>
    <li>Multiphase Flows</li>
    <li>Turbulence Flows</li>
  </ul>
</div>



  <div class="sidebar-card sidebar-tags">
    <div class="sidebar-card-title">Tags</div>
    <div class="sidebar-tagcloud">
      <a href="/tags/Actor-Critic-Method/" style="font-size: 9px;">Actor-Critic Method</a> <a href="/tags/Bellman-Equation/" style="font-size: 9px;">Bellman Equation</a> <a href="/tags/BioFluid-Mechanics/" style="font-size: 9px;">BioFluid Mechanics</a> <a href="/tags/Deep-Q-Learning/" style="font-size: 9px;">Deep Q-Learning</a> <a href="/tags/Fuild-Mechanics/" style="font-size: 9px;">Fuild Mechanics</a> <a href="/tags/Importance-Sampling/" style="font-size: 9px;">Importance Sampling</a> <a href="/tags/Monte-Carlo/" style="font-size: 9px;">Monte Carlo</a> <a href="/tags/Policy-Gradient-Method/" style="font-size: 9px;">Policy Gradient Method</a> <a href="/tags/Policy-Iteration/" style="font-size: 9px;">Policy Iteration</a> <a href="/tags/Q-Learning/" style="font-size: 12.5px;">Q-Learning</a> <a href="/tags/REINFORCE/" style="font-size: 9px;">REINFORCE</a> <a href="/tags/Reinforcement-Learning/" style="font-size: 16px;">Reinforcement Learning</a> <a href="/tags/Reinforcement-Learning%EF%BC%8CSarsa/" style="font-size: 9px;">Reinforcement Learning，Sarsa</a> <a href="/tags/Robbins-Monro-Algorithm/" style="font-size: 9px;">Robbins-Monro Algorithm</a> <a href="/tags/SARSA/" style="font-size: 12.5px;">SARSA</a> <a href="/tags/Stochastic-Approximation/" style="font-size: 9px;">Stochastic Approximation</a> <a href="/tags/Stochastic-Gradient-Descent/" style="font-size: 9px;">Stochastic Gradient Descent</a> <a href="/tags/Temporal-Difference-Algorithm/" style="font-size: 9px;">Temporal Difference Algorithm</a> <a href="/tags/Value-Function-Approximation/" style="font-size: 9px;">Value Function Approximation</a> <a href="/tags/Value-Iteration/" style="font-size: 9px;">Value Iteration</a>
    </div>
  </div>



<div class="sidebar-card sidebar-music">
  <div class="sidebar-card-title">Focus playlist · Jay Chou</div>
  <div class="sidebar-music-desc">
    Classic Jay for coding &amp; derivations.
  </div>
  <div id="aplayer-sidebar" class="sidebar-music-player"></div>
</div>


<div class="sidebar-card sidebar-runtime">
  <div class="sidebar-card-title">Site uptime</div>
  <div class="sidebar-runtime-text">
    <span id="site-runtime-days">Loading…</span>
  </div>
</div>


<link rel="stylesheet" href="https://unpkg.com/aplayer/dist/APlayer.min.css">
<script src="https://unpkg.com/aplayer/dist/APlayer.min.js"></script>

<script>
(function () {
  // DOM 就绪后再初始化
  function ready(fn) {
    if (document.readyState === 'loading') {
      document.addEventListener('DOMContentLoaded', fn);
    } else {
      fn();
    }
  }

  ready(function () {
    var container = document.getElementById('aplayer-sidebar');
    if (!container) {
      console.error('APlayer container not found');
      return;
    }
    if (!window.APlayer) {
      console.error('APlayer lib not loaded');
      return;
    }

    var player = new APlayer({
      container: container,
      fixed: false,
      mini: false,
      autoplay: false,
      theme: '#2f7dd3',
      loop: 'all',
      order: 'list',
      preload: 'metadata',
      audio: [
        {
          name: '你是我的OK绷',
          artist: 'Jay Chou',
          url: '/music/ok-beng.mp3',
          cover: '/images/ok-beng.jpg'
        },
        {
          name: '晴天',
          artist: 'Jay Chou',
          url: '/music/qingtian.mp3',
          cover: '/images/qingtian.jpg'
          },
        {
          name: '花海',
          artist: 'Jay Chou',
          url: '/music/huahai.mp3',
          cover: '/images/mojiezuo.jpg'
        },
        {
          name: '龙卷风',
          artist: 'Jay Chou',
          url: '/music/longjuanfeng.mp3',
          cover: '/images/longjuanfeng.jpg'
        },
        {
          name: '断了的弦',
          artist: 'Jay Chou',
          url: '/music/duanledexian.mp3',
          cover: '/images/duanledexian.jpg'
        },
      ]
    });

    console.log('APlayer initialized:', player);
  });

  // 站点运行天数（记得改成真实建站时间）
  var start = new Date('2025-11-08T00:00:00');
  var now = new Date();
  var days = Math.floor((now - start) / 86400000);
  var el = document.getElementById('site-runtime-days');
  if (el) {
    el.textContent = (days >= 0 ? days : 0) + ' days of notes';
  }
})();
</script>

      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2025-11 – 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Jason Huang</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">66k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">1:01</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

<div class="theme-info">
  <div class="powered-by"></div>
  <span class="post-count">Total words:  12.1k</span>
</div>


    <script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <span id="busuanzi_container_site_pv">Total site views: <span id="busuanzi_value_site_pv"></span></span>
    <span class="post-meta-divider">|</span>
    <span id="busuanzi_container_site_uv">Total visitors: <span id="busuanzi_value_site_uv"></span></span>
    <span class="post-meta-divider">|</span>
<!-- 不蒜子计数初始值纠正 -->
<script>
$(document).ready(function() {

    var int = setInterval(fixCount, 50);  // 50ms周期检测函数
    var countOffset = 20000;  // 初始化首次数据

    function fixCount() {            
       if (document.getElementById("busuanzi_container_site_pv").style.display != "none")
        {
            $("#busuanzi_value_site_pv").html(parseInt($("#busuanzi_value_site_pv").html()) + countOffset); 
            clearInterval(int);
        }                  
        if ($("#busuanzi_container_site_pv").css("display") != "none")
        {
            $("#busuanzi_value_site_uv").html(parseInt($("#busuanzi_value_site_uv").html()) + countOffset); // 加上初始数据 
            clearInterval(int); // 停止检测
        }  
    }
       	
});
</script> 


        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  
  <script color='0,0,255' opacity='0.5' zIndex='-1' count='99' src="/lib/canvas-nest/canvas-nest.min.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>


  <script defer src="/lib/three/three.min.js"></script>


  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
NexT.utils.loadComments(document.querySelector('#lv-container'), () => {
  window.livereOptions = {
    refer: location.pathname.replace(CONFIG.root, '').replace('index.html', '')
  };
  (function(d, s) {
    var j, e = d.getElementsByTagName(s)[0];
    if (typeof LivereTower === 'function') { return; }
    j = d.createElement(s);
    j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
    j.async = true;
    e.parentNode.insertBefore(j, e);
  })(document, 'script');
});
</script>
<script>
document.addEventListener('DOMContentLoaded', function () {
  var posts = document.querySelectorAll('.post-block');
  posts.forEach(function (post) {
    var link = post.querySelector('.post-title-link');
    if (!link) return;
    if (post.querySelector('.post-card-link')) return;

    var overlay = document.createElement('a');
    overlay.href = link.href;
    overlay.className = 'post-card-link';
    post.appendChild(overlay);
  });
});
</script>

<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.12"></script>
<script>
document.addEventListener('DOMContentLoaded', function () {
  var el = document.getElementById('typed-subtitle');
  if (!el) return;

  new Typed('#typed-subtitle', {
    strings: [
      "Welcome to Jason's CFD World 🚀",
      "Tracking my journey in fluid mechanics",
      "Exploring numerical methods",
      "Learning AI for scientific computing",
      "It's a public notebook, not a polished textbook"
    ],
    typeSpeed: 55,
    backSpeed: 30,
    backDelay: 1800,
    loop: true,
    smartBackspace: true,
    showCursor: true,
    cursorChar: '|'
  });
});
</script>

<script>
document.addEventListener('DOMContentLoaded', function () {
  var hero  = document.querySelector('.home-hero');
  var posts = document.querySelector('.home-posts');
  if (!hero || !posts) return;

  function onScroll() {
    var h = hero.offsetHeight || 1;
    var y = window.scrollY || window.pageYOffset || 0;

    // 0 ~ 1，表示滚动穿过 hero 的进度
    var t = Math.max(0, Math.min(y / h, 1));

    // 上面：从 1 慢慢减到 0，并轻微上移
    var heroOpacity = 1 - t * 1.2;          // 稍微快一点淡出
    heroOpacity = heroOpacity < 0 ? 0 : heroOpacity;
    hero.style.opacity   = heroOpacity;
    hero.style.transform = 'translateY(' + (-20 * t) + 'px)';

    // 下面：从 0 慢慢到 1，同时从下往上滑进来
    var postsOpacity = Math.max(0, Math.min(t * 1.4, 1));
    posts.style.opacity   = postsOpacity;
    posts.style.transform = 'translateY(' + (24 * (1 - postsOpacity)) + 'px)';
  }

  onScroll();
  window.addEventListener('scroll', onScroll, { passive: true });
});
</script>


</body>
</html>
