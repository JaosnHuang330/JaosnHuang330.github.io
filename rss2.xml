<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Jason&#39;s CFD World</title>
    <link>http://example.com/</link>
    
    <atom:link href="http://example.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>Explore the Beauty of Flows</description>
    <pubDate>Tue, 11 Nov 2025 08:32:58 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>BioFluid Mechanics - Lung and Respiration System</title>
      <link>http://example.com/2025/11/10/BioFluid1/</link>
      <guid>http://example.com/2025/11/10/BioFluid1/</guid>
      <pubDate>Mon, 10 Nov 2025 11:13:33 GMT</pubDate>
      
      <description>This note is just the Question and Answer of LUNG ADN RESPIRATION SYSTEM.</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Lung-and-Respiration-system"><a href="#Lung-and-Respiration-system" class="headerlink" title="Lung and Respiration system"></a>Lung and Respiration system</h1><ol><li><details> The main function of respiratory system is to supply oxygenated blood to living organs and to remove carbon dioxide.   <summary> What is the main function of resipratory system?</li><li><details> 1. pulmonary ventilation; 2. diffusion of O2 and CO2 inside alveolo of the lung; 3. transportation of O2 and CO2 to and from the body tissue. 4. regulation of ventilation.  <summary> Explain four major functions of the respiratory system.</li><li><details> It first flows through nasal cavity, then it passes through pharynx, trachea, bronchus, bronchiole, and finally the alveolo.   <summary> Please describe the flow path of atmospheric air in the respiration system.</li><li><details> There are two lungs in the respiratory system and total 23th bifurcations. The right one is slightly bigger than the left one due to middle mediastinum. The bifurcation will increase the surface area for gas exchange. The zeroth bifurcation is <strong>trachea</strong>; the first bifurcation is <strong>brochi</strong>. The 11th generation is <strong>brochioles</strong>; The 16th generation is <strong>avleoli</strong>strong>, which can exchange gas with capillary. The 23 generation is called <strong>alveoli sac</strong>.   <summary> Describe the general morpholoy of respiration system.    </li><li><details>  <summary> Describe the overall structure of human lung. </summary>  There are upper airway, Tracheo-bronchial Tree and Gas Exchange at alveoli. For Upper airway, there are Nose: nostril, nasal cavity (two channels); Pharynx: nose-mouth-glottis, velum; Larynx: division of gut, trachea, epoglottis, glottis. For tracheo-bronchial tree, there are trachea, main bronchus, right lung (56% with 3 lobes) and left lung (44% with 2 lobes). For gas exchange at alveoli, alveoli in the alveolar sacs are matched with capillaries.</details>  </li><li><details>  <summary> What are the three zones of human lung? </summary>  <ol>    <li>Conducting Zone: there is no alveoli. It consists of bronchi and bronchioles, directly down to terminal bronchiole (TB). Bulk flow is always in there.  The main function of that zone is to regulate the temperature, moisterize, capture the particle through (cilia and mucus)</li>    <li>Transition Zone is also the respiratory bronchiole (RB). </li>    <li> Respiratory Zone consists of alveolar duct (AD) and alveolar sac (AS). There is no cillia/mucus but there is surfactant. The blood vessels are mainly populated in that region. AS gas-exchange membrane and blood flows in capillaries. Blood passes through seperated air spaces between two sheets. </li>  </ol></li><li><details>  <summary> Describe Alveoli </summary>  First, there are 3 to the 10th power of alveoli in an adult's lung. The diameter varies widely 160-350 micrometer; the entrance opening is smaller than overall diameter. The <strong>Alveolar septum</strong> is a capillary wall between two sheet of epothelia</details></li><li><details>  <summary>Describe the morphogenesis of human lung </summary>  Week 4 hung bud outgrows from a spot in intestine; form pharnyx, trachea, bronchi and internal epithlium; Single bud contect centrally to differentiate between gut and airway; Airway tube forms trachea which is divided into two bronchial buds. Week5 main trachea is formed, braching into secondary bronchi(2, 3); then lobes are formed; tertiary/segmental bronchi are formed. Week16 terminal bronchioles (TB) is formed. Month 6 each TB divides into more htan 2RB, which divide into 3-6 alveolar ducts. Month 7 terminal sacs and alveoli are formed. Epithelia change from cubical to planar shape, capillaries are foremd. In this stage, the immature baby can be survivable. However, the conducting tree is not fully developed after birth. Alveoli is totally immature. 10% at birth, 50% year 1. completed at year 8.</details></li><li><details>  <summary> What are the main functions of humnan lung?</summary>  <ol>    <li> Gas Exchange: lung try to maximize gas exchange area for a given blood flow. 1ml of fresh blood carries 0.2ml of O2: only 4% CO2 carries in blood is expelled at the lung. 70%->10% O2 in vein; Gaseous CO2 5%, 88% HCO3 (26 in RBC, 62 in plasma); 250ml/min at rest of O2 consumption</li>    <li> Breathing: It is induced by the motion of diaphram and rib; diaphram is a thin sheet that seperates abdominal and thoracic cavities.(convex upward at rest, flat during inhalation) </li>    <li> Blood acidity: acidity is important to cell function. When blood is too acid or alkaline, nural function is ditributed, than coma, muscle spasm, death. The acidity is mainly determined by CO2 concentration</li>  </ol></details></li><li><details>  <summary> What are the flow characteristics in human lung</summary>  <ol>    <li> General: 1. Steady-Unsteady; 2. Cilia and Surfactant surface have no effect on air flowl; 3. Cartilage in large brochi affects air flow; 4. Turbulent jet formed in trachea past larnyx 5. change in airway diameter during breathing is negligible </li>    <li> Upper airway: 1. Laminar Flow from nostril to internal ostium 2. Sudden area increase after IO, inducing flow seperation and turbulent flow 3. Turbulence remains all over the main cavity'; two nasal cavities turn 90 downward. </li>    <li> mean axial velocity: 1m/s at trachea, 1mm/s avleoli, the profile will be skewed.</li>  </ol></details></li><li><details>  <summary> What are the four factors that affects the gas exchange?</summary>  1. the thickness and surface area of the membrane; 2. the diffusion coefficient of the gas and the pressure difference betweenm the two sides of the alveolar memebrane. 3. The bindings rate of O2 to hemoglobin 4.the volume of blood in the hung capillaries. </details></li><li><details>  <summary> Two important factors in the gas tranporation and exchange in alveoli capillaries </summary>  1. the local concentration  2. total gas flux </details></li><li><details>   <summary> Briefly summarize the pariticle depostion</summary>  <ol>    <li> Particle concentration is closely correlated withd death rate </li>    <li> Decomposition of PM (particle matter)</li>        <ol>         <li> Big particles are deposited on TB, while the small one in AS by diffusion. The minimum deposition is 0.5 um; Velocity profile is much important; The Re effect is higher in 3D than in 2D. </li>        <li> non-uniform harzard hot spots are determined by the highest local concentration caused by inhalation and exhalation. when d > 10um, the particles can b e removed at nose</li>        <li> PM will grow due to huminity </li>      </ol>    <li> Respiratory flow can affect the particle decomposition: 1) flow in trachea turbulent (Re-2100) can enhance deposition of particles; Glottis jet: particles are collected along the center, and collide on carina. 3) Irregularity on the airway can also suppress turbulence. </li>    <li> The perticle can be removed minutes at nose, bronchi for hours, and alveoli for weeeks; Deposited PMs on mucus move toward throat by cilia + gas flow in and out. </li>    <li> If using drug, there is less side effect, fast absorption.</li>  </ol></li></ol>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/BioFluid-Mechanics/">BioFluid Mechanics</category>
      
      
      <category domain="http://example.com/tags/BioFluid-Mechanics/">BioFluid Mechanics</category>
      
      <category domain="http://example.com/tags/Fuild-Mechanics/">Fuild Mechanics</category>
      
      
      <comments>http://example.com/2025/11/10/BioFluid1/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>RL Note 8: Actor-Critic Method</title>
      <link>http://example.com/2025/11/10/RL-note8/</link>
      <guid>http://example.com/2025/11/10/RL-note8/</guid>
      <pubDate>Sun, 09 Nov 2025 20:05:26 GMT</pubDate>
      
      <description>This note wiil introduce Actor-Critic Method, which can basically seen as the combination of Value-based method and Policy-based method.</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Actor-Critic-Method"><a href="#Actor-Critic-Method" class="headerlink" title="Actor-Critic Method"></a>Actor-Critic Method</h1><blockquote><p>It’s the combination of <strong>value-based and policy-based algorithm</strong>. But it is still in the scope of <strong>policy gradient algorithm</strong>. Actor is for policy update, while critic is the value update step.</p>$$\theta_{t+1} = \theta_t + \alpha \nabla_\theta J(\theta_t) =\theta_t + \alpha \mathbb{E}_{S\sim \eta, A \sim \pi(S, \theta)}[\nabla_\theta \ln \pi (A|S, \theta )q_\pi(S, A)]$$$$\theta_{t+1}  =\theta_t + \alpha [\nabla_\theta \ln \pi (a_t|s_t, \theta )q_t(s_t, a_t)]$$</blockquote><p>When the $q_t(s_t, a_t)$ is estimated by TD Learning, the algorithm will be <em><strong>actor-critic. This shows the combination of  value-based and policy-based methods,</strong> as $\theta$ appears in $\pi$ and $q_\pi$.</em> </p><ul><li>The <em><strong>critic</strong></em> corresponds to the <em><strong>value update step via the Sarsa algorithm.</strong></em></li><li>The <strong>actor</strong> corresponds to the <strong>policy update step.</strong></li></ul><p>Since $q_t(s_t, a_t)$ is unknown, we can classfify the methods based on the approaches to use to approximate the $q_t(s_t,a_t)$</p><ul><li>If we use <strong>MC learning</strong>, then the method is called <strong>REINFORCE.</strong></li><li>if we use <strong>Temporal Difference Learning</strong>, then the method is called <strong>Actor-Critic</strong></li></ul><h3 id=""><a href="#" class="headerlink" title=""></a></h3><p><strong>Algorithm</strong>: QAC</p><p><strong>Initialization</strong>:  A policy function is initialized as $\pi(a|s, \theta_0)$; An action value function is initialized as $q(s, a, w_0)$.</p><p><strong>Goal</strong>: to minimize the objective function $J(\theta)$</p><p>At time step t in each episode</p><ul><li><p>Generate $a_t$ by following $\pi(a_t|s_t,\theta_t)$, observing $s_{t+1}, r_{t+1}$ and generate $a_{t+1}$ by following $\pi(a_{t+1}|s_{t+1}, \theta_t)$.</p></li><li><p><strong>Actor (Policy Update)</strong></p></li></ul><p>$$<br>\theta_{t+1}  &#x3D;\theta_t + \alpha [\nabla_\theta \ln \pi (a_t|s_t, \theta )q_t(s_t, a_t)]<br>$$</p><ul><li><strong>Critic (Value Update)</strong></li></ul><p>$$<br>w_{t+1} &#x3D; w_k + \alpha_k(r_{t+1} + \gamma \hat {q}(s_{t+1}, a_{t+1}, w_t) - \hat{q}(s_t, a_t,  w_t))\nabla_w \hat{q}(s_t, a_t, w_t)<br>$$</p><h2 id="Advantage-Actor-Critic-A2C"><a href="#Advantage-Actor-Critic-A2C" class="headerlink" title="Advantage Actor-Critic (A2C)"></a>Advantage Actor-Critic (A2C)</h2><h3 id="Baseline-Invariance"><a href="#Baseline-Invariance" class="headerlink" title="Baseline Invariance"></a>Baseline Invariance</h3>$$\mathbb{E}_{S\sim \eta, A \sim \pi(S, \theta)}[\nabla_\theta \ln \pi (A|S, \theta )q_t(S, A)]  = \mathbb{E}_{S\sim \eta, A \sim \pi(S, \theta)}[\nabla_\theta \ln \pi (A|S, \theta )(q_t(S, A) - b(S))]$$<p>The <em><strong>baseline</strong></em> is a scalar function of $S$. It is useful since it can reduce the <strong>approximation variance</strong>, when we use samples to approximate the true gradient. </p><p>The optimal baseline to minimize $\text{var}(X)$  is </p> $$b^*(s) = \frac{\mathbb{E}_{\mathcal{A} \sim \pi}[||\nabla_\theta \ln \pi(A|s, \theta_t)||^2\ q_\pi(s, A)]}{\mathbb{E}_{\mathcal{A} \sim \pi}[||\nabla_\theta \ln \pi(A|s, \theta_t)||^2]}, \quad s\in \mathcal{S}$$However, we always use $$b^{s} = \mathbb{E}_{A\sim\pi}[{q_\pi(s, A)}] = v_\pi(s)$$Therefore the gradient ascent becomes$$\theta_{t+1}  =\theta_t + \alpha \mathbb{E}_{S\sim \eta, A \sim \pi(S, \theta)}[\nabla_\theta \ln \pi (A|S, \theta )(q_t(S, A) - v_t(S))] \\=  \theta_t + \alpha \mathbb{E}_{S\sim \eta, A \sim \pi(S, \theta)}[\nabla_\theta \ln \pi (A|S, \theta )\delta_t(S, A)]$$$\delta _\pi(S, A)$ is called advantage function. This an be approximated by TD errors.$$q_t(s_t, a_t)- v_t(s_t) \approx r_{t+1} + \gamma v_t(s_{t+1}) - v_t(s_t)$$Since we use the TD errors,  this method is also called TD-AC.<p><strong>Algorithm</strong>: Q2C</p><p><strong>Initialization</strong>:  A policy function is initialized as $\pi(a|s, \theta_0)$; A value function $v(s, w_0)$ where $w_0$ is the initial parameter. .</p><p><strong>Goal</strong>: to minimize the objective function $J(\theta)$</p><p>At time step t in each episode, do</p><ul><li><p>Generate $a_t$ by following $\pi(a_t|s_t,\theta_t)$, observing $s_{t+1}, r_{t+1}$.</p></li><li><p><strong>Advantage (TD error)</strong>: </p><p>$\delta_t &#x3D; r_{t+1} + \gamma v(s_{t+1}, w_t) - v(s_t, w_t)$</p></li><li><p><strong>Actor (Policy Update)</strong></p></li></ul><p>$$<br>\theta_{t+1}  &#x3D;\theta_t + \alpha \delta_t \nabla_\theta \ln\pi(a_t|s_t, \theta_t)<br>$$</p><ul><li><strong>Critic (Value Update)</strong></li></ul><p>$$<br>w_{t+1} &#x3D; w_k + \alpha_k \delta_t  \nabla_w v(s_t,w_t)<br>$$</p><h3 id="Off-Policy-AC"><a href="#Off-Policy-AC" class="headerlink" title="Off-Policy AC"></a>Off-Policy AC</h3><h3 id="Importance-Sampling"><a href="#Importance-Sampling" class="headerlink" title="Importance Sampling"></a>Importance Sampling</h3><p>Our goal is to estimate $\mathbb{E}_{X\sim p_0}[X]$. </p>Suppose we have random variable $X \in \mathcal{X}$, and we have its probability distribution $p_0(X)$. If the sample $\{x_i\}_{i=1}^{n}$ are generated by following the distribution $p_0$, then we can use the mean estimation $\bar x$  to approximate the $\mathbb{E}_{X\sim p_0}[X]$.However, when the sample $\{x_i\}_{i=1}^{n}$ is not generated from $p_0$, but generted from $p_1$, what should we do?In this case, we can use importance sampling technique to approximate it. $$\mathbb{E}_{X\sim p_0}[X] = \sum_{x \in \mathcal{X}} p_0(x)x = \sum_{x \in \mathcal{X}}p_1(x)\frac{p_0(x)}{p_1(x)}x = \mathbb{E}_{X\sim p_1}[f(X)]$$$$\mathbb{E}_{X\sim p_0}[X] = \mathbb{E}_{X\sim p_1}[f(X)] \approx \bar{f} = \frac{1}{n} \sum_{i=1}^n\frac{p_0(x_i)}{p_1(x_i)}x_i$$where the $\frac{p_0(x_i)}{p_1(x_i)}$ is the importance weight<h2 id="Off-policy-policy-gradient-Theorem"><a href="#Off-policy-policy-gradient-Theorem" class="headerlink" title="Off-policy policy gradient Theorem"></a>Off-policy policy gradient Theorem</h2>$$J(\theta) = \sum_{s\in \mathbb{S}} d_\beta(s)v_\pi(s) = \mathbb{E}_{\mathbb{S \sim d_\beta}}[v_\pi(S)]$$$$\nabla_\theta J(\theta) = \mathbb{E}_{\mathcal{S}\sim \rho, A\sim \beta}[\frac{\pi(A|S, \theta)}{\beta(A|S)}\nabla_\theta \ln(A|S, \theta)q_\pi(S, A)]$$where the $\frac{\pi(A|S, \theta)}{\beta(A|S)}$ is the importance weight. $$\rho (s)= \sum_{s'\in \mathcal{S}}d_{\beta}(s')\text{Pr}_\pi(s|s')$$After we add baseline to reduce the estimation variance.$$\nabla_\theta J(\theta) = \mathbb{E}_{\mathcal{S}\sim \rho, A\sim \beta}[\frac{\pi(A|S, \theta)}{\beta(A|S)}\nabla_\theta \ln(A|S, \theta)(q_\pi(S, A)-v_\pi(S))]$$The corresponding SGA is$$\theta_{t+1} = \theta_{t} + \frac{\pi(a_t|s_t, \theta)}{\beta(a_t|s_t)}\delta_t\nabla_\theta \ln(a_t|s_t, \theta)$$<h2 id="Deterministic-Actor-Critic"><a href="#Deterministic-Actor-Critic" class="headerlink" title="Deterministic Actor-Critic"></a>Deterministic Actor-Critic</h2><blockquote><p>Deterministic Case is naturally off-policy and effectively handle <strong>continuous action spaces</strong>.  Here, $a &#x3D; \mu(s, \theta)$ is used to denote a deterministic policy.</p></blockquote>$$\nabla_\theta J(\theta) = \sum_{s\in \mathcal{S}} d_\mu(s)\nabla_\theta \,\mu(s)(\nabla_a q_\mu (s, a))|_{a = \mu(s)} = \mathbb{E}_{\mathcal{S} \sim d_{\mu}}[\nabla_\theta \mu(S)(\nabla_a q_\mu(S, a))|_{a=\mu(S)}]$$<p>The corresponding stochastic gradient-ascent algorithm is<br>$$<br>\theta_{t+1} &#x3D; \theta_{t} +\alpha_\theta\nabla_\theta \mu(s_t)(\nabla_a q_\mu(s_t, a))|_{a&#x3D;\mu(s_t)}<br>$$</p><p>DAC is off-policy, as the behavior policy $d_\mu$ maybe is different from the target policy $\mu$. The actor is off-policy, since there is no $A$ in the expression. The critic is off-policy as the the first policy that generate $a_t$ by interacting with the environment, which is then the behavior policy, while the second policy is $u$, which is the target policy the critic aims to evaluate. </p><p><strong>Algorithm: Deterministic policy gradient or deterministic actor-critic</strong></p><p><strong>Initialization</strong>: A given behavior policy $\beta(a|s)$. A target policy $\pi(a|s,\theta_0)$. </p><p>At time step $t$ in each episode, do </p><p>Generate $a_t$ following $\beta(s_t)$ and observe $r_{t+1},s_{t+1}$</p><p>Advantage:</p><p>$\delta_t &#x3D; r_{t+1} + \gamma v(s_{t+1}, w_t) - v(s_t, w_t)$</p><p>Actor (Policy Update):</p><p>$$<br>\theta_{t+1} &#x3D; \theta_{t} + \frac{\pi(a_t|s_t, \theta)}{\beta(a_t|s_t)}\delta_t\nabla_\theta \ln(a_t|s_t, \theta)<br>$$</p><p>Critic (Value Update):</p><p>$$<br>w_{t+1} &#x3D; w_t + \alpha_w \delta_t \nabla_w q(s_t, a_t, w_t)<br>$$</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Reinforcement-Learning/">Reinforcement Learning</category>
      
      
      <category domain="http://example.com/tags/Reinforcement-Learning/">Reinforcement Learning</category>
      
      <category domain="http://example.com/tags/Actor-Critic-Method/">Actor-Critic Method</category>
      
      <category domain="http://example.com/tags/Importance-Sampling/">Importance Sampling</category>
      
      
      <comments>http://example.com/2025/11/10/RL-note8/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>RL Note 7: Policy Gradient Method</title>
      <link>http://example.com/2025/11/10/RL-note7/</link>
      <guid>http://example.com/2025/11/10/RL-note7/</guid>
      <pubDate>Sun, 09 Nov 2025 19:55:56 GMT</pubDate>
      
      <description>This note will illustrate the transition from value-based methods to policy-based methods. REINFORCE algorithm is introduced as well.</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Policy-Gradient-Method"><a href="#Policy-Gradient-Method" class="headerlink" title="Policy Gradient Method"></a>Policy Gradient Method</h1><blockquote><p>value-based → policy-based. In this case, the policy can be written as $\pi (a|s, \theta)$, where $\theta \in \mathbb{R^m}$ is a parameter vector. This means that we can also use function to represent the policy. Function-represented policies optimal policies can be obtained through policy gradient method, which is policy-based method. It is more efficent for handling larget spaces. </p></blockquote><ol><li>select a metric.</li><li>optimize it through using <strong>gradient-ascent algorithm</strong>.</li></ol><p>​<img src="/2025/11/10/RL-note7/figure1.png" class=""></p><ul><li><p><strong>Metric 1: Average state value</strong></p>  $$  \bar{v}_\pi = \sum_{s\in \mathcal{S}}d(s)v_\pi(s)  $$    <p>  where $d(s)$ is the weight of state $s$. It satisfies $d(s) \geq 0$ for any $s \in \mathcal{S}$ and $\sum_{s\in \mathcal{S}}d(s)&#x3D;1$</p><p>  If we interpret $d(s)$ as the probability distribution of  $s$, The metric will be  </p>  $$  \bar{v}_\pi = \mathbb{E}_{S \sim d}[v_\pi(S)]  $$    <p>  We can treat all the states equally important by selecting the $d_0(s) &#x3D; 1&#x2F;|\mathcal S|$ or we can only interest in only one state $d_0(s_0) &#x3D; 1, d_0(s\neq 0) &#x3D; 0$. If the $d$  is dependent with policy $\pi$ then we could use the stationary distribution. </p><p>$$<br>d_\pi^T P_\pi &#x3D;d_\pi^T<br>$$</p><p>  The metrics can also be defined like this</p>  $$  J(\theta) = \lim_{n\rightarrow \infty}\mathbb{E}[\sum_{t=0}^n \gamma^t R_{t+1}] = \mathbb{E}[\sum_{t=0}^{\infty}\gamma^tR_{t+1}]  $$    $$  \mathbb{E}[\sum_{t=0}^{\infty}\gamma^tR_{t+1}] = \sum_{s\in \mathcal{S}}{d(s)}\mathbb{E}[\sum_{t=0}^{\infty}\gamma^t R_{t+1}|S_0=s] = \sum_{s\in \mathcal{S}}d(s)v_\pi(s)  = \bar{v}_\pi = d^T v_\pi  $$​    </li><li><p><strong>Metrics 2: Average reward</strong></p></li></ul><p>​$$\bar{r}_\pi = \sum_{s\in \mathcal{S}}d_\pi (s)r_\pi(s) = \mathbb{E}_{S\sim d_\pi}[r_\pi(S)]$$​</p><p>​here the $d_\pi$  is the stationary distribution<br>$$<br>r_\pi(s) &#x3D; \sum_{a \in \mathcal{A}} \pi(a|s, \theta)r(s, a) &#x3D; \mathbb{E}_{\mathcal{A}\sim \pi(s, \theta)}[r(s, A)|s]<br>$$</p><p>​here $r(s, a) &#x3D; \mathbb{E}[R|s, a] &#x3D; \sum_r rp(r|s, a)$</p><p>$$<br>J(\theta) &#x3D; \lim_{n\rightarrow\infty} \frac{1}{n} \mathbb{E} [\sum_{t&#x3D;0}^{n-1 }R_{t+1}]<br>$$</p><p>$$<br>J(\theta) &#x3D; \lim_{n\rightarrow\infty} \frac{1}{n} \mathbb{E} [\sum_{t&#x3D;0}^{n-1 }R_{t+1}] &#x3D; \sum_{s\in \mathcal{S}}d_{\pi}(s)r_{\pi}(s) &#x3D; \bar{r}_\pi<br>$$</p><h3 id="Policy-Gradient-Theorem"><a href="#Policy-Gradient-Theorem" class="headerlink" title="Policy Gradient Theorem"></a>Policy Gradient Theorem</h3><p>$$<br>\nabla_{\theta}J(\theta) &#x3D; \sum_{s\in \mathcal{S}}\eta(s)\sum_{a\in\mathcal{A}}\nabla_\theta \pi(a|s, \theta)q_\pi(s, a)<br>$$</p><p>The compact form in terms of expectation is </p>$$\nabla_\theta J(\theta)  = \mathbb{E}_{S\sim \eta, A \sim \pi(S, \theta)}[\nabla_\theta \ln \pi (A|S, \theta )q_\pi(S, A)]$$<h3 id="Derivation-of-the-gradients-in-the-discounted-case"><a href="#Derivation-of-the-gradients-in-the-discounted-case" class="headerlink" title="Derivation of the gradients in the discounted case"></a>Derivation of the gradients in the discounted case</h3><p>$$<br>v_\pi (s) &#x3D; \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \cdots | S_t &#x3D; s]<br>$$</p><p>$$<br>q_\pi(s, a) &#x3D; \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \cdots | S_t &#x3D; s, A_t &#x3D; a]<br>$$</p>It holds that $v_\pi(s) = \sum_{a\in \mathcal{A}}\pi(a|s) q_\pi(s, a)$. We can also shows that $\bar{r}_\pi(\theta)$ and $\bar{v}_\pi(\theta)$ are the same expression. $$\bar{r}_\pi = (1 - \gamma )\bar{v}_\pi$$Know that $\bar{v}_\pi(\theta) = d_\pi^Tv_\pi$ and $\bar{r}_\pi (\theta) = d_\pi^Tr_\pi$, and the Bellman equation is $v_\pi =r_\pi + \gamma P_\pi v_\pi$. $$\bar{v}_\pi  = \bar{r}_\pi + \gamma d_\pi^T P_\pi v_\pi = \bar{r}_\pi+ \gamma \bar{v}_\pi$$<p>​<img src="/2025/11/10/RL-note7/figure2.png" class=""></p><p>​<img src="/2025/11/10/RL-note7/figure3.png" class=""></p><h3 id="MC-Policy-Gradient-REINFORCE"><a href="#MC-Policy-Gradient-REINFORCE" class="headerlink" title="MC Policy Gradient (REINFORCE)"></a>MC Policy Gradient (REINFORCE)</h3><p>The gradient ascent to maximizing $J(\theta)$ is </p>$$\theta_{t+1} = \theta_t + \alpha \nabla_\theta J(\theta_t) =\theta_t + \alpha \mathbb{E}_{S\sim \eta, A \sim \pi(S, \theta)}[\nabla_\theta \ln \pi (A|S, \theta )q_\pi(S, A)]$$<p>Apply SGD<br>$$<br>\theta_{t+1}  &#x3D;\theta_t + \alpha [\nabla_\theta \ln \pi (a_t|s_t, \theta )q_t(s_t, a_t)]<br>$$</p><p>where $q_t(s_t,a_t)$  is the approximation of $q_\pi(s_t,a_t)$ . If it is obtained by MC, the algorithm is called REINFORCE.</p><p>$$<br>\theta_{t+1}  &#x3D;\theta_t + \alpha (\frac{q_t(s_t,a_t)}{\pi(a_t|s_t, \theta_t)})\nabla_\theta \pi(a_t|s_t, \theta)<br>$$<br><strong>Algorithm</strong>: REINFORCE (Policy Gradient by MC Learning )</p><p><strong>Initialization</strong>:  Initial parameter $\theta$ ; $\gamma \in (0, 1)$ ; $\alpha &gt; 0$</p><p><strong>Goal</strong>: Learn an optimal policy for maximizing $J(\theta)$</p><p>For each episode, do</p><ul><li><p>Generate an episode ${s_0, a_0, r_1, \cdots, s_{T-1}, a_{T-1}, r_T}$ following $\pi(\theta)$</p></li><li><p>For $t &#x3D; 0, 1, \cdots, T-1$</p><ul><li>Value update: $q_t(s_t, a_t) &#x3D; \sum_{k&#x3D;t+1}^T \gamma^{k-t+1} r_k$</li><li>Policy update: $\theta \leftarrow \theta +  \alpha \nabla_\theta \ln \pi(a_t|s_t, \theta)q_t(s_t, a_t)$</li></ul></li></ul>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Reinforcement-Learning/">Reinforcement Learning</category>
      
      
      <category domain="http://example.com/tags/Policy-Gradient-Method/">Policy Gradient Method</category>
      
      <category domain="http://example.com/tags/Reinforcement-Learning%EF%BC%8CSarsa/">Reinforcement Learning，Sarsa</category>
      
      <category domain="http://example.com/tags/REINFORCE/">REINFORCE</category>
      
      
      <comments>http://example.com/2025/11/10/RL-note7/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>RL Note 6: Value Function Approximation</title>
      <link>http://example.com/2025/11/10/RL-note6/</link>
      <guid>http://example.com/2025/11/10/RL-note6/</guid>
      <pubDate>Sun, 09 Nov 2025 19:50:04 GMT</pubDate>
      
      <description>This note wiil introduce the transition from table-based methods to function-based method.</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Value-Function-Approximation"><a href="#Value-Function-Approximation" class="headerlink" title="Value Function Approximation"></a>Value Function Approximation</h1><blockquote><p>The state&#x2F;action values are represented as <strong>function</strong> for <strong>large scale state and action spaces</strong>. <strong>The NN can be applied from this chapter.</strong> There are two problems when considering function approximation: 1) which feature vector should be chosen 2) which parameter vector is the best for approximation.</p></blockquote><ol><li>select an objective function for defining optimal policies. </li><li>derive gradient of the objective function.</li><li>apply gradient-based algorithm to solve the optimization problem.</li></ol><h2 id="Define-Objective-Function"><a href="#Define-Objective-Function" class="headerlink" title="Define Objective Function"></a>Define Objective Function</h2><p>$$<br>J(w) &#x3D; \mathbb{E}[(v_{\pi}(S)- \hat v(S, w))^2]<br>$$</p><p>where $v_{\pi}(s)$ is the <em><strong>true state value</strong></em>, $\hat v(s, w)$ is the <em><strong>approximated state value</strong></em>. Our goal is to find an optimal $w$  that can best approximate the $v_\pi(s)$ for every $s$. </p><ul><li>What is the probability distribution of $S$</li></ul><p>First is the <strong>uniform distribution</strong>: (no consideration of long-term Markov process)</p><p>$$<br>J(w) &#x3D; \frac{1}{n}\sum_{s\in \mathcal{S}}[(v_{\pi}(s)- \hat{v}(s, w))^2]<br>$$</p><p>Second is the <strong>stationary distribution</strong>: (consider the long-term behavior of MDP)</p><p>Let ${d_\pi(s)}_{s\in \mathcal S}$ is the stationary distribution of the Markov Process under policy $\pi$. </p><p>$$<br>J(w) &#x3D; \sum_{s\in \mathcal{S}}d_\pi(s)[(v_{\pi}(s)- \hat{v}(s, w))^2]<br>$$</p><p>To obtain $d_\pi (s)$ we need the transitional probability. The $d_\pi$ can be calculated in this way.<br>$$<br>d_\pi^T &#x3D; d_\pi^T P_\pi<br>$$<br>Here the $d_s$ is the left eigenvector of $P_\pi$ associated with the eigenvalue 1. The solution of this is called the stationary distribution. </p><h3 id="Minimize-the-Objective-Function"><a href="#Minimize-the-Objective-Function" class="headerlink" title="Minimize the Objective Function"></a>Minimize the Objective Function</h3><p>We can apply Gradient Descent Algorithm:</p><p>$$<br>w_{k+1} &#x3D; w_k - \alpha_k \nabla_w J(w_k)<br>$$</p><p>$$<br>\nabla_w J (w_k) &#x3D; -2 \mathbb{E}[(v_\pi(S) - \hat{v}(S, w_k))(\nabla_w \hat{v}(S, w_k))]<br>$$</p><p>$$<br>w_{k+1} &#x3D; w_k + 2\alpha_k  \mathbb{E}[(v_\pi(S) - \hat{v}(S, w_k))(\nabla_w \hat{v}(S, w_k))]<br>$$</p><p>There is a expectation in terms of $S$, we can use SGD instead:</p><p>$$<br>w_{t+1} &#x3D; w_t + \alpha_t  (v_\pi(s_t) - \hat{v}(s_t, w_t))(\nabla_w \hat{v}(s_t, w_t))<br>$$</p><p>But here $v_\pi(s)$ is unknown. </p><ul><li><p>We can use MC Method: $g_t$ can be used as an approximation of $v_\pi(s_t)$.</p><p>  $$<br>  w_{t+1} &#x3D; w_t + \alpha_t(g_t- \hat{v}(s_t, w_t))\nabla_w\hat{v}(s_t, w_t)<br>  $$</p></li><li><p>Time Temporal Method: $v_\pi(s_t) &#x3D; r_{t+1} + \gamma \hat {v}(s_{t+1}, w_t)$</p></li></ul><p>$$<br>w_{t+1} &#x3D; w_k + \alpha_k(r_{t+1} + \gamma \hat {v}(s_{t+1}, w_t) - \hat{v}(s_t, w_t))\nabla_w \hat{v}(s_t, w_t)<br>$$</p><p><strong>Algorithm</strong>: TD learning of state values with function approximation</p><p><strong>Initialization</strong>: A function $\hat{v}(s, w)$ that is a differentiable in $w$. Initial parameter $w_0$. </p><p><strong>Goal</strong>: Learn the true state values of a <strong>given policy</strong> $\pi$</p><p>For each episode ${(s_t, r_{t+1}, s_{t+1})}_t$ generated by $\pi$, do </p><ul><li>For each sample $(s_t, r_{t+1}, s_{t+1})$, do<ul><li>In the general case,  $w_{t+1} &#x3D; w_k + \alpha_k(r_{t+1} + \gamma \hat {v}(s_{t+1}, w_t) - \hat{v}(s_t, w_t))\nabla_w \hat{v}(s_t, w_t)$</li></ul></li></ul><h3 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h3><p>$$<br>w_{t+1} &#x3D; w_k + \alpha_k(r_{t+1} + \gamma \hat {q}(s_{t+1}, a_{t+1}, w_t) - \hat{q}(s_t, a_t,  w_t))\nabla_w \hat{q}(s_t, a_t, w_t)<br>$$</p><p><strong>Algorithm: SARSA with function approximation</strong></p><p><strong>Initialization:</strong> Initial parameter $w_0$. Initial policy $\pi_0$. $\alpha_t &#x3D; \alpha &gt; 0$ for all $t$</p><p><strong>Goal:</strong> Learn an optimal policy that can lead the agent to the target from an initial state $s_0$. </p><p>For each episode, do </p><ul><li><p>Generate $a_0$ at $s_0$  following $\pi_0 (s_0)$</p></li><li><p>If $s_t$ $(t&#x3D;0,1,2,\cdots)$ is not the target state, do </p><ul><li><p>Collect the experience sample $(a_{t+1}, s_{t+1}, r_{t+1})$ given $(s_t, a_t)$: generate $r_{t+1}, s_{t+1}$</p></li><li><p>by interacting with the environment: generate $\pi_{t}(s_{t+1})$.</p></li><li><p><strong>Update q-value</strong></p><ul><li>$w_{t+1} &#x3D; w_k + \alpha_k(r_{t+1} + \gamma \hat {q}(s_{t+1}, a_{t+1}, w_t) - \hat{q}(s_t, a_t,  w_t))\nabla_w \hat{q}(s_t, a_t, w_t)$</li></ul></li><li><p><strong>Update policy</strong></p><ul><li>$\epsilon$-greedy</li><li>$s_t \leftarrow s_{t+1}$, $a_t \leftarrow a_{t+1}$</li></ul></li></ul></li></ul><h3 id="Q-Learning-with-function-approximation"><a href="#Q-Learning-with-function-approximation" class="headerlink" title="Q-Learning with function approximation"></a>Q-Learning with function approximation</h3><p>$$<br>w_{t+1} &#x3D; w_k + \alpha_k(r_{t+1} + \gamma \max_{a\in\mathcal{A(s_{t+1})}}\hat {q}(s_{t+1}, a_{t}, w_t) - \hat{q}(s_t, a_t,  w_t))\nabla_w \hat{q}(s_t, a_t, w_t)<br>$$</p><p><strong>Algorithm</strong>: on-policy Q-Learning with function approximation</p><p><strong>Initializaiton</strong>: $\alpha_t(s, a) &#x3D; \alpha &gt; 0$ for all $(s, a)$ and all $t$. $\epsilon \in (0, 1)$ . Initial $q_0(s, a)$ for all $(s, a)$. </p><p><strong>Goal</strong>: Learn an optimal path that can lead the agent ot the target state from an intial state $s_0$.</p><p>For each episode, do </p><ul><li><p>if $s_t$ is not the target state, do </p><ul><li><p>Collect the experience sample $(a_t, r_{t+1}, s_{t+1})$ given $s_t$ </p></li><li><p>Update q-value for $(s_t, a_t)$ </p>$w_{t+1}(s_t, a_t) = w_t(s_t, a_t)+\alpha_t(s_t, a_t)[(r_{t+1}+ \gamma \max_{a\in\mathcal{A}(s_{t+1})}\hat{q}_t(s_{t+1},a_t, w_t) - \hat{q}_t(s_t,a_t,w_t)]\nabla_w \hat{q}(s_t, a_t, w_t) $ </li><li><p>Update policy for $s_t$:</p><ul><li>Use the $\epsilon$-greedy policy</li></ul></li></ul></li></ul><h2 id="Deep-Q-Learning"><a href="#Deep-Q-Learning" class="headerlink" title="Deep Q-Learning"></a>Deep Q-Learning</h2><p>The DQN aims to minimize the following objective function: </p><p>$$<br>J &#x3D;\mathbb{E}[(R + \gamma \max_{a\in \mathcal{A(S’)}}\hat{q}(S’, a, w)-\hat{q}(S, A, w))^2]<br>$$</p><p>$$<br>q(s, a) &#x3D; \mathbb{E}[R_{t+1} + \gamma \max_{a \in \mathcal{A}(S_{t+1})}q(S_{t+1}, a)], \quad  \text{for all } s, a<br>$$</p><p>where $(S, A, R, S’)$ are random variable. The objective function can be viewed as squared Bellman optimality error. </p><p>There are two networks for the DQN: <strong>target network  $\hat{q}(s, a, w_T)$</strong> which is used to obtain the <strong>target value $y_T&#x3D; r + \gamma \max_{a\in \mathcal{A}(s’)}\hat{q}(s’,a, w_T)$</strong> and <strong>main network $\hat{q}(s, a, w)$</strong> is used to minimized <strong>the target error.</strong> </p><p>$$<br>J &#x3D; \mathbb{E}[(R+\gamma \max_{a\in\mathcal{A}(S’)}\hat{q}(S’, a, w_T)-\hat{q}(S, A, w))^2]<br>$$</p><p>where, $w_T$ is the target network’s parameter. When $w_T$ is fixed, the gradient of $J$ is </p><p>$$<br>\nabla_w J &#x3D; \mathbb{E}[(R + \gamma \max_{a\in \mathcal{A}(S’)}\hat{q}(S’, a, w_T)-\hat{q}(S, A, w))\nabla_w\hat{q}(S, A, w)],<br>$$</p><p><strong>Algorithm Deep Q-learning</strong></p><p><strong>Initialization</strong>: <strong>A main network</strong> and <strong>a target network</strong> with the same initial parameter. </p><p><strong>Goal</strong>: Learn an optimal target network to approximate the optimal action values from the experience samples generated by a given behavior policy $\pi_b$. </p><p>Store the experience samples generated by $\pi_b$ in a replay buffer $\mathcal{B} &#x3D; {s, a, r, s’}$</p><ul><li><p>For each iteration, do </p><ul><li><p>Uniformly draw a mini-batch of samples from $\mathcal{B}$</p></li><li><p>For each sample $(s, a, r, s’)$, calculate the target value $y_T$. </p></li><li><p>Update the main network to minimize the $\delta$ using the mini-batch of samples</p></li></ul></li><li><p>Set $w_T &#x3D; w$ every C iterations.</p></li></ul><p>Minimize the TD errors, which is a metric to measure the convergence and the performance of a model.</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Reinforcement-Learning/">Reinforcement Learning</category>
      
      
      <category domain="http://example.com/tags/Reinforcement-Learning/">Reinforcement Learning</category>
      
      <category domain="http://example.com/tags/SARSA/">SARSA</category>
      
      <category domain="http://example.com/tags/Q-Learning/">Q-Learning</category>
      
      <category domain="http://example.com/tags/Value-Function-Approximation/">Value Function Approximation</category>
      
      <category domain="http://example.com/tags/Deep-Q-Learning/">Deep Q-Learning</category>
      
      
      <comments>http://example.com/2025/11/10/RL-note6/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>RL Note 5: Temporal Difference Algorithm</title>
      <link>http://example.com/2025/11/10/RL-note5/</link>
      <guid>http://example.com/2025/11/10/RL-note5/</guid>
      <pubDate>Sun, 09 Nov 2025 19:34:38 GMT</pubDate>
      
      <description>This note will introduce TD Algorithm and it&#39;s application in SARSA and Q-Learning.</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Temporal-Difference-Algorithm"><a href="#Temporal-Difference-Algorithm" class="headerlink" title="Temporal Difference Algorithm"></a>Temporal Difference Algorithm</h1><blockquote><p>It’s the stochastic gradient descent for solving Bellman or Bellman Optimality Equation. <strong>Model-Free Algorithm.</strong> TD can be used to solve Bellman Equation and Bellman Optimality Equation.</p></blockquote><h2 id="TD-Algorithm"><a href="#TD-Algorithm" class="headerlink" title="TD Algorithm"></a>TD Algorithm</h2><p>It estimates the <em><strong>state values</strong></em> of a given policy. </p><p>Given a policy $\pi$, our goal is to estimate $v_\pi (s)$ for all $s \in \mathcal{S}$. Suppose that we have some experience samples $(s_0, r_1, s_1, \cdots, s_t, r_{t+1}, s_{t+1}, \cdots)$ generated following $\pi$. Here, $t$ denotes the time step and $v_t(s_t)$ is the estimate of $v_\pi(s_t)$. </p><p>$$<br>v_{t+1}(s_t) &#x3D; v_t(s_t) - \alpha_t(s_t)[v_t(s_t) - (r_{t+1} + \gamma v_t(s_{t+1}))]<br>$$</p><p>$$<br>v_{t+1}(s) &#x3D; v_t(s), \quad \text{for all} \quad s \neq s_t<br>$$</p><p>Only the visited state $s_t$ will be updated, other will remian the same as before. </p><p>The definition of the state value is </p><p>$$<br>v_\pi(s) &#x3D; \mathbb{E}[R_{t+1}+ \gamma G_{t+1}|S_t &#x3D; s], \quad s \in \mathcal{S}<br>$$</p><p>$$<br>v_\pi(s) &#x3D; \mathbb{E}[R_{t+1}+ \gamma v_{\pi}(S_{t+1})|S_t &#x3D; s], \quad s \in \mathcal{S}<br>$$</p><p>The equation (4) is also called the expected bellman equation.</p><p>The Bellman Equaiton (4) can be wriiten by using RM algortithm as<br>$$<br>g(v_\pi(s_t)) &#x3D; v_\pi(s_t) - \mathbb{E}[R_{t+1}+ \gamma v_{\pi}(S_{t+1})|S_t &#x3D; s] &#x3D;0<br>$$</p><p>The noisy observation can be written as</p><p>$$<br>\tilde{g}(v_\pi(s_t))&#x3D;v_\pi -  [r_{t+1}+ \gamma v_\pi (s_{t+1})] &#x3D; g(v_\pi(s_t)) + \eta<br>$$</p><p>$$<br>v_{t+1 (s_t)} &#x3D; v_t(s_t) - \alpha_t(s_t)\tilde{g}(v_t(s_t))<br>$$</p><h3 id="Property-of-TD-Algorithm"><a href="#Property-of-TD-Algorithm" class="headerlink" title="Property of TD Algorithm"></a>Property of TD Algorithm</h3><p>The TD Algorithm is defined as </p><p>$$<br>v_{t+1}(s_t) &#x3D; v_t(s_t) - \alpha_t(s_t)[v_t(s_t) - (r_{t+1} + \gamma v_t(s_{t+1}))]<br>$$</p><p>where $\bar{v}_t$ is the TD target</p><p>$$<br>\bar{v}<em>t &#x3D; r</em>{t+1} + \gamma v_t(s_{t+1})<br>$$</p><p>It is called TD target is because $\bar v_t$ is the target value, the algorithm attempts to drive $v(s_t)$  to. </p><p>$$<br>v_{t+1}(s_t) - \bar v_t &#x3D; [v_t(s_t) - \bar v_t] - \alpha_t(s_t)[v_t(s_t) - \bar v_t]<br>$$</p><p>$$<br>v_{t+1}(s_t) - \bar v_t &#x3D; [1 - \alpha_t(s_t)][v_t(s_t) - \bar v_t]<br>$$</p><p>$$<br>|v_t(s_t) - \bar v_t| \geq |v_{t+1}(s_t) - \bar v_t|<br>$$</p><p>which means that the updated state value is closer to the target value than the old state value. </p><p>$\delta_t$ is the TD error</p><p>$$<br>\delta_t &#x3D; v(s_t) - \bar{v_t} &#x3D; v_t(s_t) - (r_{t+1} + \gamma v_t(s_{t+1}))<br>$$</p><p>which is also known as the <em><strong>temporal-difference</strong></em> because it reflects the discrepancy between two steps $t$ and $t + 1$. The <strong>TD error is zero if the state value estimate is accurate</strong>. When $v_t &#x3D; v_\pi$, the expected value of the TD error is </p><p>$$<br>\mathbb{E}[\delta_t|S_t &#x3D; s_t] &#x3D; \mathbb{E}[v_\pi (S_t) - (R_{t+1}+\gamma v_\pi (S_{t+1}))|S_t &#x3D; s_t]&#x3D;v_\pi(s_t) - \mathbb{E}[R_{t+1} + \gamma v_{\pi}(S_{t+1})|S_t &#x3D; s_t]&#x3D;0<br>$$</p><p>It can also be interpreted as innovation.  </p><h2 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h2><p>Rather than estimating the state value like TD algorithm, SARSA estimates the <em><strong>action values</strong></em> of a given policy.  Therefore, SARSA is also known as the TD algorithm for Action Value. However, to obtain the optimal policy. SARSA have to combine with policy improvement method. </p><p>Suppose we have some experience samples generated following $\pi: (s_0, a_0, r_1, a_1, \cdots, s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}, \cdots)$ </p><p>$$<br>q_{t+1}(s_t, a_t) &#x3D; q_t(s_t, a_t) - \alpha_t(s_t, a_t)[q_t(s_t, a_t) - (r_{t+1} + \gamma q_t(s_{t+1},a_{t+1}))]<br>$$</p><p>$$<br>q_{t+1}(s, a) &#x3D; q_t(s,a), \quad \text{for all } (s, a) \neq (s_t, a_t)<br>$$</p><p>Here the $v_t(s_t, a_t)$ is the estimation of $v_\pi(s_t, a_t)$. At time t, only the q-value of $(s_t, a_t)$ is updated, whereas the q-values of the others remain the same. The SARSA can be used to solve the <em><strong>Bellman Equation</strong></em> expressed in terms of <em><strong>action values</strong></em>. </p><p>$$<br>q_\pi (s, a) &#x3D; \mathbb{E}[R + \gamma q_\pi(S’, A’)|s, a], \quad \text{for all } (s, a)<br>$$</p><p><strong>Algorithm</strong>: First, update the q-value of the visited action-state pair. Second, update the policy to an $\epsilon$-greedy one. </p><p><strong>Initialization: $\alpha_t (s, a) &#x3D; \alpha &gt; 0$</strong>  for all $(s, a)$ and all $t$. Initial $q_0(s, a)$ for all $(s, a)$. Initial $\epsilon$-greedy policy $\pi_0$ derived from $q_0$. </p><p><strong>Goal:</strong> Learn an <em><strong>optimal policy</strong></em> that can lead the agent to the target state from an initial state $s_0$. </p><p>For each episode, do </p><ul><li><p>Generate $a_0$ at $s_0$ following $\pi_0(s_0)$</p></li><li><p>If $s_t (t&#x3D;0, 1, 2,\cdots)$ is not the target state, do </p><ul><li><p>Collect an experience sample $(r_{t+1}, s_{t+1}, a_{t+1})$ given $(s_t, a_t)$: generate $r_{t+1}$, $s_{t+1}$</p></li><li><p>by interacting with the environments, generate $a_{t+1}$ following $\pi_t(s_{t+1})$. </p></li><li><p><em><strong>Update q-value</strong></em> for $(s_t, a_t)$:</p><p>​    $q_{t+1}(s_t, a_t) &#x3D; q_t(s_t, a_t) - \alpha_t(s_t, a_t)[v_t(s_t, a_t) - (r_{t+1} + \gamma v_t(s_{t+1},a_{t+1}))]$</p></li><li><p><em><strong>Update policy</strong></em> for $s_t$:</p><ul><li>Use $\epsilon$-greedy policy</li><li>$s \leftarrow s_{t+1}, a_t \leftarrow a_{t+1}$</li></ul></li></ul></li></ul><p>The task of SARSA is to find an optimal path from a <em><strong>specific starting state</strong></em> to a <em><strong>target state</strong></em>. <strong>Therefore, we don’t need to consider all the states</strong>. </p><p>There are two strategies to evaluate the convergence and performance of SARSA: <strong>Episode Length, Total Rewards</strong>. </p><h2 id="N-step-SARSA"><a href="#N-step-SARSA" class="headerlink" title="N-step SARSA"></a>N-step SARSA</h2><p><em><strong>SARSA</strong></em> and <em><strong>MC learning</strong></em> are two extreme cases of <em><strong>n-step SARSA</strong></em>. The main difference is how many steps are taken to obtain the action value $G_t$.<br>$$<br>q_{t+1}(s_t, a_t) &#x3D; q_t(s_t, a_t) - \alpha_t(s_t, a_t)[v_t(s_t, a_t) - (r_{t+1} + \gamma r_{t+2} + \gamma^2r_{t+3}+\cdots + \gamma^nq_t(s_{t+n}, a_{t+n})]<br>$$</p><p>The definition of the action value is </p><p>$$<br>q_\pi(s, a) &#x3D; \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a]<br>$$</p><p>Where $G_t$ is the discounted return satisfying </p><p>$$<br>G_t &#x3D; R_{t+1} + \gamma R_{t+2}+ \gamma^2 R_{t+3} + \cdots<br>$$</p><h3 id="Sarsa-1"><a href="#Sarsa-1" class="headerlink" title="Sarsa"></a>Sarsa</h3><p>$$<br>G_t &#x3D; G_{t}^{(1)} &#x3D; R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1})<br>$$</p><h3 id="n-step-Sarsa"><a href="#n-step-Sarsa" class="headerlink" title="n-step Sarsa"></a>n-step Sarsa</h3><p>$$<br>G_t &#x3D; G_t^{(n)} &#x3D; R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^n q_\pi(S_{t+n}, A_{t+n})<br>$$</p><h3 id="MC-Learning"><a href="#MC-Learning" class="headerlink" title="MC Learning"></a>MC Learning</h3><p>$$<br>G_t &#x3D; G_t^{(\infty)}&#x3D; R_{t+1} + \gamma R_{t+2}+ \gamma^2 R_{t+3} + \cdots<br>$$</p><h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><p>Directly solve the <strong>Bellman Optimality Equation</strong> and <strong>estimate optimal action values</strong> to obtain the optimal policy. This is also the <em><strong>off-policy</strong></em> Algorithm. <em><strong>SARSA</strong></em> solves the <em><strong>Bellman Equation</strong></em>, while <em><strong>Q-Learning</strong></em> solves the <em><strong>Bellman Optimality Equation</strong></em>. </p><p>$$<br>q_{t+1}(s_t, a_t) &#x3D; q_t(s_t, a_t)-\alpha_t(s_t, a_t)[q_t(s, a)-(r_{t+1}+ \gamma \max_{a\in\mathcal{A}(s_{t+1})}q_t(s_{t+1}, a))]<br>$$</p><p>$$<br>q_{t+1}(s, a)&#x3D; q_t(s, a), \quad \text{for all } (s, a) \neq (s_t, a_t)<br>$$</p><p>Here, the $q_t(s_t)$ is the optimal action value of $(s_t, a_t)$. The TD target $\bar v_t &#x3D; r_{t+1}+ \gamma \max_{a\in\mathcal{A}(s_{t+1})}q_t(s_{t+1}, a)$. The Sarsa requires $(s_{t+1}, r_{t+1},a_{t+1})$, while the Q-learning merely requires $(s_{t+1}, r_{t+1})$. The Q-learning aims to solve <em><strong>the Bellman Optimality Equation in terms of action value, which is expressed as followed.</strong></em> </p><p>$$<br>q(s,a) &#x3D; \mathbb{E}[R_{t+1} + \gamma\max_a q(S_{t+1}, a)|S_t &#x3D;s, A_t&#x3D;a]<br>$$</p><p><strong>Algorithm</strong>: on-policy Q-Learning </p><p><strong>Initializaiton</strong>: $\alpha_t(s, a) &#x3D; \alpha &gt; 0$ for all $(s, a)$ and all $t$. $\epsilon \in (0, 1)$ . Initial $q_0(s, a)$ for all $(s, a)$. </p><p><strong>Goal</strong>: Learn an optimal path that can lead the agent ot the target state from an intial state $s_0$.</p><p>For each episode, do </p><ul><li><p>if $s_t$ is not the target state, do </p><ul><li><p>Collect the experience sample $(a_t, r_{t+1}, s_{t+1})$ given $s_t$ </p></li><li><p>Update q-value for $(s_t, a_t)$ </p><ul><li>$q_{t+1}(s_t, a_t) &#x3D; q_t(s_t, a_t)-\alpha_t(s_t, a_t)[q_t(s, a)-(r_{t+1}+ \gamma \max_{a\in\mathcal{A}(s_{t+1})}q_t(s_{t+1}, a))]$</li></ul></li><li><p>Update policy for $s_t$:</p><ul><li>Use the $\epsilon$-greedy policy</li></ul></li></ul></li></ul><p><strong>Algorithm</strong>: off-policy Q-Learning </p><p><strong>Initializaiton</strong>: $\alpha_t(s, a) &#x3D; \alpha &gt; 0$ for all $(s, a)$ and all $t$. $\epsilon \in (0, 1)$ . Initial $q_0(s, a)$ for all $(s, a)$. </p><p><strong>Goal</strong>: learn an optimal target policy $\pi_T$ for all states from the experence sampels generated by $\pi_b$.</p><p>For each episode genrated by $\pi_b$, do </p><ul><li><p>if $s_t$ is not the target state, do </p><ul><li><p>Collect the experience sample $(a_t, r_{t+1}, s_{t+1})$ given $s_t$ </p></li><li><p>Update q-value for $(s_t, a_t)$ </p><ul><li>$q_{t+1}(s_t, a_t) &#x3D; q_t(s_t, a_t)-\alpha_t(s_t, a_t)[q_t(s, a)-(r_{t+1}+ \gamma \max_{a\in\mathcal{A}(s_{t+1})}q_t(s_{t+1}, a))]$</li></ul></li><li><p>Update policy for $s_t$:</p><ul><li>Use the $\epsilon$-greedy policy</li></ul></li></ul></li></ul>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Reinforcement-Learning/">Reinforcement Learning</category>
      
      
      <category domain="http://example.com/tags/Temporal-Difference-Algorithm/">Temporal Difference Algorithm</category>
      
      <category domain="http://example.com/tags/SARSA/">SARSA</category>
      
      <category domain="http://example.com/tags/Q-Learning/">Q-Learning</category>
      
      
      <comments>http://example.com/2025/11/10/RL-note5/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>RL Note 4: Stochastic Approximation</title>
      <link>http://example.com/2025/11/10/RL-note4/</link>
      <guid>http://example.com/2025/11/10/RL-note4/</guid>
      <pubDate>Sun, 09 Nov 2025 19:05:30 GMT</pubDate>
      
      <description>This note will mainly cover stochastic apprximation approaches, while introducing RM Algorithm and SGD Algorithm.</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Stochastic-Approximation"><a href="#Stochastic-Approximation" class="headerlink" title="Stochastic Approximation"></a>Stochastic Approximation</h1><blockquote><p>There are two approaches to calculate the $\bar x$, <em><strong>non-incremental</strong></em> and <em><strong>incremental</strong></em>. Stochasitc Approximation is an approach that use incremental method rather than non-incremental method for root-finding or optimization problems. </p></blockquote><p><strong>Mean Estimation Example</strong></p><p>$$<br>\mathbb{E}[X] \approx \bar x &#x3D; \frac{1}{n}\sum_{j&#x3D;1}^{n}x_j, \quad n\rightarrow\infty, \quad \bar x \rightarrow \mathbb{E}[X]<br>$$</p><p>Suppose that </p><p>$$<br>w_{k+1} &#x3D; \frac{1}{k}\sum_{i&#x3D;1}^{k}x_i, \quad k &#x3D; 1, 2,\cdots<br>$$</p><p>$$<br>w_k &#x3D; \frac{1}{k-1}\sum_{i&#x3D;1}^{k-1}x_i, \quad k&#x3D;2, 3, \cdots<br>$$</p><p>$$<br>w_{k+1} &#x3D; \frac{1}{k}\sum_{i&#x3D;1}^{k}x_i &#x3D; \frac{1}{k}(\sum_{i&#x3D;1}^{k-1}x_i + x_k) &#x3D; \frac{1}{k}((k-1)w_k+x_k)&#x3D; w_k - \frac{1}{k}(w_k-x_k)<br>$$</p><p>Therefore, we obtain the iterative form</p><p>$$<br>w_{k+1} &#x3D; w_k - \frac{1}{k}(w_k - x_k)<br>$$</p><p>There is a general expression of equation (5), which is quite similar to the following RM algorithm.  </p><p>$$<br>w_{k+1} &#x3D; w_k - \alpha_k(w_k - x_k), \alpha_k &gt;0<br>$$</p><p>When the $\alpha_k$ statisfies some conditions, $w_k \rightarrow \mathbb{E}[X]$  as $k \rightarrow \infty$</p><h2 id="Robbins-Monro-Algorithm"><a href="#Robbins-Monro-Algorithm" class="headerlink" title="Robbins-Monro Algorithm"></a>Robbins-Monro Algorithm</h2><blockquote><p>It’s one of the typical <em><strong>Stochastic Algorithms</strong></em> (<em><strong>stochastic iterative algorithms</strong></em>) for <em><strong>solving root-finding or optimization problems</strong></em>.  Stochastic Gradient Descent (SGD) is a special form of RM.</p></blockquote><p>Considering a Black-Box System, here only the input $w$ and the noisy output $\tilde{g}(w, \eta)$ are known. <strong>The expression of function $g$ is unknown.</strong> Our aim is to solve $g(w) &#x3D; 0$ using $w$ and $\eta$. The RM algorithm for the aim is defined as<br>$$<br>\tilde{g}(w, \eta) &#x3D; g(w) + \eta<br>$$</p><p>$$<br>w_{k+1} &#x3D; w_k - a_k \tilde{g}(w_k, \eta_k), \quad k &#x3D; 1, 2, \cdots<br>$$</p><p>We simply consider the case of $w_{k+1} &#x3D; w_k - a_k g(w_k)$, for $g(w) &#x3D; \text{tanh}(w-1)$. The iterative expression will lead to convergence of $w^* &#x3D; 1$. e.g. for $g(w)&#x3D;\tanh(w-1)$</p><ul><li>$w_k &gt; w^<em>$ , $g(w_k)&gt;0$. Then $w_{k+1}&lt;w_k$. When the $k$  is large enough. $w_{\infin} \rightarrow w^</em>$</li><li>$w_k &lt; w^<em>$ , $g(w_k)&gt;0$. Then $w_{k+1}&lt;w_k$. When the $k$  is large enough. $w_{\infin} \rightarrow w^</em>$</li></ul><h3 id="RM-theorem"><a href="#RM-theorem" class="headerlink" title="RM theorem"></a>RM theorem</h3><p>If </p><ol><li>$0&lt; c_1 \leq \nabla_w g(w)\leq c_2 \quad \text{for all}$ $w$</li></ol><ol start="2"><li><p>$\sum_{k&#x3D;1}^\infty a_k &#x3D; \infty$, $\sum_{k&#x3D;1}^{\infty}a_k^2 &lt; \infty$   </p></li><li><p>$\mathbb{E}[\eta_k|\mathcal{H}_k] &#x3D; 0$  and $\mathbb{E}[\eta_k^2|\mathcal{H}_k] &#x3D; \infty$ where  $\mathcal{H}_k= \{ w_k, w_{k-1}, \cdots\}$, </p></li></ol><p>then $w_k$ surely converges to the root $w^*$. </p><p>The second condition is very important as</p><p>$$<br>w_{k+1} - w_k &#x3D; -a_k \tilde{g}(w_k, \eta_k), \quad k &#x3D; 1, 2, \cdots<br>$$</p><ul><li>The $\sum_{k&#x3D;1}^{\infty}a_k^2 &lt; \infty$ means that if $k\rightarrow \infty$, then $a_k \rightarrow 0$, then $w_{k+1} &#x3D; w_{k}$.</li><li>The $\sum_{k&#x3D;1}^\infty a_k &#x3D; \infty$ means that $a_k$  should not converge too fast. we have $w_1 - w_\infty\sum_{k&#x3D;1}^{\infty}a_k \tilde{g}(w_k, \eta_k)$. If $\sum_{k&#x3D;1}^\infty a_k &#x3D; \infty$, then $\sum_{k&#x3D;1}^{\infty}a_k \tilde{g}(w_k, \eta_k)$ is bounded. Therefore, $|w_1 - w_\infty| \leq b$.</li></ul><p>For example, we can define a funciton as $g(w) &#x3D; w - \mathbb{E}[X]$. The noisy observation can be defined like $\tilde{g}(w, \eta) &#x3D; w - x$, if noisy observation $x$ and input $w$ is given.</p><p>$\tilde{g}(w, \eta) &#x3D; w - x &#x3D; w - x + \mathbb{E}[X] - \mathbb{E}[X]&#x3D; w - \mathbb{E}[X] + \mathbb{E}[X]- x &#x3D; g(w) + \eta$</p><p>$\eta &#x3D; \mathbb{E}[X]- x$, $g(w) &#x3D; w - \mathbb{E}[X]$.   Therefore, the RM algorithm for solving this problem is<br>$$<br>w_{k+1} &#x3D; w_k - \alpha_k \tilde{g}(w_k, \eta_k) &#x3D; w_k - \alpha_k(w_k - x_k)<br>$$</p><h2 id="Stochastic-Gradient-Descent-Algorithm"><a href="#Stochastic-Gradient-Descent-Algorithm" class="headerlink" title="Stochastic Gradient Descent Algorithm"></a>Stochastic Gradient Descent Algorithm</h2><p>Consider the following optimization problem:</p><p>$$<br>\min_w J(w) &#x3D; \mathbb{E}[f(w, X)]<br>$$</p><p><strong>This algorithm can be translated as finding a parameter $w$ , which could minimize the expected loss in all the possible samples.</strong></p><p>where the $w$ is parameter needed to be optimized, and the $X$ is random variable, the expectation will be calculated based on $X$. </p><p>We can use the <em><strong>gradient descent</strong></em> to solve the optimized $w^*$. The gradient of $\mathbb{E}[f(w, X)]$ is $\nabla_w \mathbb{E}[f(w, X)]$, which is equivalent to  $\mathbb{E}[\nabla_w f(w, X)]$. Then the gradient descent is </p><p>$$<br>w_{k+1} &#x3D; w_k - \alpha_k \nabla_w J(w_k) &#x3D; w_k - \alpha_k \mathbb{E}[\nabla_w f(w, X)]<br>$$</p><p>For model-free cases, we can calculate this through using a large number of iid samples ${x_i}_{i&#x3D;1}^n$, the gradient of the expected value is </p><p>$$<br>\mathbb{E}[\nabla_w f(w, X)]\approx \frac{1}{n}\sum_{i&#x3D;1}^{n}\nabla_wf(w_k, x_i)<br>$$</p><p>Thus, the gradient descent is </p><p>$$<br>w_{k+1} &#x3D; w_k - \frac{\alpha_k}{n}\sum_{i&#x3D;1}^{n}\nabla_wf(w_k, x_i)<br>$$</p><p>For these algorithms, we have to collect all the samples in advanced, which is low efficient. Then we can introduce SGD instead. </p><p>$$<br>w_{k+1} &#x3D; w_k - \alpha_k\nabla_wf(w_k, x_k)<br>$$</p><p>The SGD replaces the true gradient $\mathbb{E}[\nabla_w f(w, X)]$ with stochastic gradient $\nabla_w f(w, X)$.</p><p>$$<br>\nabla_wf(w_k, x_k) &#x3D; \mathbb{E}[\nabla_w f(w, X)] + \nabla_wf(w_k, x_k)- \mathbb{E}[\nabla_w f(w, X)] \<br>&#x3D; \mathbb{E}[\nabla_w f(w, X)] + \eta_k<br>$$</p><p>Therefore, the SGD can be written as<br>$$<br>w_{k+1} &#x3D; w_k - \alpha_k \mathbb{E}[\nabla_wf(w, X)]-\alpha_k \eta_k<br>$$<br>This is the same as regualar GD, except the extra $\alpha_k \eta_k$. </p><p>Since,<br>$$<br>\mathbb{E}[\eta_k] &#x3D; \mathbb{E}[\nabla_wf(w_k, x_k)- \mathbb{E}[\nabla_w f(w, X)]] &#x3D; \mathbb{E}_{x_k}[\nabla_wf(w_k, x_k)] - \mathbb{E}_X[\nabla_wf(w_k, X)] &#x3D; 0<br>$$</p><p>as $x_k$ is iid, the additional $\eta$ will not affect the convergence of iteration. </p><h3 id="Convergence-Pattern-of-SGD"><a href="#Convergence-Pattern-of-SGD" class="headerlink" title="Convergence Pattern of SGD"></a>Convergence Pattern of SGD</h3><blockquote>When $w_k$ is close to $w^*$, does the convergence become more random. But when the $w_k$ is far away from the $w^*$, The SGD will behave similarly to the regular gradient descent. </blockquote><p>We can define the <em><strong>relative error</strong></em> between stochastic <strong>gradient descent</strong> and <strong>regular gradient descent</strong>. </p>$$\delta_k = \frac{|\nabla_wf(w_k, x_k)-\mathbb{E}[\nabla_w f(w_k, X)]|}{\mathbb{E}[\nabla_w f(w_k, X)]}$$which can also be written as $$\delta_k = \frac{|\nabla_wf(w_k, x_k)-\mathbb{E}[\nabla_w f(w_k, X)]|}{\mathbb{E}[\nabla_w f(w_k, X)-\mathbb{E}[\nabla_w f(w^*, X) ]} = \frac{|\nabla_wf(w_k, x_k)-\mathbb{E}[\nabla_w f(w_k, X)]|}{\mathbb{E}[\nabla_w^2 f(\tilde{w}, X)(w_k - w^*)]}$$since $\mathbb{E}[\nabla_w^2 f(\tilde{w}, X)(w_k - w^*)] = 0$ and mean value theorom. $\tilde{w}_k \in [w_k, w^*]$.$$|\mathbb{E}[\nabla_w^2 f(\tilde{w}, X)(w_k - w^*)]| \geq c|w_k - w^*|$$$$\delta_k \leq \frac{|\nabla_wf(w_k, x_k)-\mathbb{E}[\nabla_w f(w_k, X)]|}{c|w_k - w^*|}$$The denominator indicates the distance between $w_k$ and optimal solution $w^*$]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Reinforcement-Learning/">Reinforcement Learning</category>
      
      
      <category domain="http://example.com/tags/Reinforcement-Learning/">Reinforcement Learning</category>
      
      <category domain="http://example.com/tags/Stochastic-Approximation/">Stochastic Approximation</category>
      
      <category domain="http://example.com/tags/Robbins-Monro-Algorithm/">Robbins-Monro Algorithm</category>
      
      <category domain="http://example.com/tags/Stochastic-Gradient-Descent/">Stochastic Gradient Descent</category>
      
      
      <comments>http://example.com/2025/11/10/RL-note4/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>RL Note 3: Monte Carlo Estimation</title>
      <link>http://example.com/2025/11/09/RL-note3/</link>
      <guid>http://example.com/2025/11/09/RL-note3/</guid>
      <pubDate>Sun, 09 Nov 2025 05:12:38 GMT</pubDate>
      
      <description>This note will mainly talk about the principle of MC Learning.</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Monte-Carlo-Estimation"><a href="#Monte-Carlo-Estimation" class="headerlink" title="Monte Carlo Estimation"></a>Monte Carlo Estimation</h1><blockquote><p>First Introduction of <strong>Model-Free Estimation Algorithm</strong>. The dilemma of Data and Model. The <em><strong>mean estimation</strong></em> is the core of MC Estimation, which <strong>uses stochastic experience samples to solve estimation problem</strong>.</p></blockquote><h2 id="Mean-Estimation"><a href="#Mean-Estimation" class="headerlink" title="Mean Estimation"></a>Mean Estimation</h2><p>Consider the a random variable $X$ which can take values from a finite set $\mathcal X$. There are two approaches to calculate the estimation of $\mathbb{E}[X]$, model-based and model-free. $\mathbb{E}[X]$ can be called as the expected value, mean value, and average. </p><ol><li><p>model-based </p><p> $$<br> \mathbb{E}[X] &#x3D; \sum_{x \in \mathcal{X}} p(x) x<br> $$</p></li><li><p>model-free (the samples have to be iid</p><p> $$<br> \mathbb{E}[X] \approx \bar x &#x3D; \frac{1}{n} \sum_{j&#x3D;1}^{n} x_j,<br> \quad n\rightarrow\infty,\quad \bar x \rightarrow \mathbb{E}[X]<br> $$</p></li></ol><h2 id="MC-Basic"><a href="#MC-Basic" class="headerlink" title="MC Basic"></a>MC Basic</h2><p>It’s important to know the fundamental idea of MC-based Reinforcement Learning. </p><p>For model-based model, the action value is:</p><p>$$<br>q_\pi (s, a) &#x3D; \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a] &#x3D; \sum_{r\in \mathcal{R}}p(r|s, a)r+ \sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s, a)<br>$$</p><p>For model-free model, the action value is the expected return when starting from $(s, a)$. </p><p>$$<br>q_\pi (s, a) &#x3D; \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a]&#x3D;\mathbb{E}[R_{t+1}+ \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots |S_t&#x3D;s, A_t&#x3D;a]<br>$$</p><p>Starting from $(s, a)$, the agent can interact with the environment by <strong>following policy $\pi_k$ and obtain certain number of episodes.</strong> Suppose there are $n$ episodes and the return for each episode is $g_{\pi_k}^{(i)}(s, a)$, then the $q_\pi (s, a)$ can be expressed as</p><p>$$<br>q_{\pi_k} (s, a) &#x3D; \mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a] &#x3D; \frac{1}{n}\sum_{i&#x3D;1}^n g_{\pi_k}^{(i)}(s, a)<br>$$</p><p><strong>Algorithm</strong> </p><p><strong>Initialization</strong>: Initial guess $\pi_0$.</p><p><strong>Goal</strong>: Search for an optimal policy. </p><p><strong>Loop</strong>: For the kth iteration, do</p><ul><li><p>For every state $s \in \mathcal{S}$ in state space, do</p><ul><li>For every action $a \in \mathcal{A}$ in action space, do<ul><li>Collecting sufficiently many episodes starting from $(s, a)$ by following $\pi_k$</li><li><strong>Policy Evaluation</strong>:</li><li>$q_{\pi_k} (s,a) \approx q_k(s, a) &#x3D;$  the average return of all the episodes starting from $(s, a)$</li></ul></li><li><strong>Policy improvement</strong>:</li><li>$a_k^*(s) &#x3D; arg \max_a q_k(s, a)$</li><li>$\pi_{k+1}(a|s) &#x3D; 1$ if $a &#x3D; a_k^*$ and $\pi_{k+1}(a|s)&#x3D;0$ otherwise</li></ul></li></ul><p>where we can also define the <em><strong>episode length</strong></em>: The length of episode will decide whether an agent can <strong>reach the target</strong> or <strong>get positive reward</strong>.  which also means that if the episode is too small, the return can not be positive so is the <strong>expected state value</strong>. </p><p>The episode length will greatly impact the final policies.  </p><h3 id="sparse-reward"><a href="#sparse-reward" class="headerlink" title="sparse reward"></a>sparse reward</h3><p>which refers to the scenario <strong>in which no positive rewards can be obtained unless the target is reached</strong>. Therefore, when the state space is too large, it will downgrade the computational efficiency. To solve this problem, we can apply <em><strong>non-sparse reward,</strong></em> which is the setting with “attractive field”, the field except for target which also has positive reward. </p><h2 id="MC-Exploring-Start"><a href="#MC-Exploring-Start" class="headerlink" title="MC Exploring Start"></a>MC Exploring Start</h2><h3 id="Initial-Visit"><a href="#Initial-Visit" class="headerlink" title="Initial Visit"></a>Initial Visit</h3><p>An episode is only used to estimate the <em><strong>action value</strong></em> of the <em><strong>initial state-action pair</strong></em> that the episode starts from.  This is the strategy of “initial visit” used by <em><strong>MC Basic</strong></em>. </p><p>e.g. Suppose there is an episode  $s1 \xrightarrow{a_2} s3 \xrightarrow{a_1} \cdots$</p><p>This episode is only used to estimate $q_t(s1, a2)$, this is sample wasting. There are many state-action pairs wasted in an episode. </p><h3 id="first-visit"><a href="#first-visit" class="headerlink" title="first-visit"></a>first-visit</h3><p>If only counting the first-time visits. The <strong>samples are approximately independent,</strong> the variance will be smaller in this case. However, the <strong>trajectory of every</strong> $(s, a)$ only has one sample. </p><h3 id="every-visit"><a href="#every-visit" class="headerlink" title="every-visit"></a>every-visit</h3><p>It can <strong>count every visit of a state-action pair.</strong> <em>If an episode is sufficiently long to visit every state-action pair multiple times, this sigle episode then is sufficient to estimalte all action values</em>. This is the best for sample usage efficiency. The number of sample is larger, which result in a faster convergence speed. But the sample are highly correlated to each other. </p><p>There are two strategies to <strong>update the policies</strong>.:</p><ul><li>In the policy evaluation step, collecting all the episodes <strong>starting from</strong> the <strong>same state-action pair</strong> and using the <strong>average return of these episode</strong>. (used by MC Basic)</li><li>Use <strong>the return of single episode</strong> to approximate the <strong>corresponding action value</strong> and improve the policy in an episode-by-episode fashion.</li></ul><p><strong>Algorithm 5.2: MC Exploring Starts</strong></p><p><strong>Initialization</strong>: Initial Policy $\pi_0(a|s)$  and initial value $q(s, a)$ for all $(s, a)$.  $\text{Returns}(s, a)&#x3D;0$ and $\text{Nums}(s, a)&#x3D;0$.  </p><p><strong>Goal</strong>: Search for an optimal policy. </p><p><strong>Loop</strong>: For each episode, do</p><ul><li><p>Episode generation: Selecting a starting state-action pair $(s_0, a_0)$ and ensure that all pairs can be selected. Following the current policy, generate an episode of length T. </p></li><li><p>Initialization for each episode: g ← 0.</p></li><li><p>For each episode, $t &#x3D; T-1, T-2, \cdots, 0$, do</p><ul><li><p>$g \leftarrow \gamma g + r_{t+1}$</p></li><li><p>Returns $(s_t, a_t) \leftarrow$  Returns $(s_t, a_t) + g$</p></li><li><p>Nums $(s_t, a_t) \leftarrow$  Nums $(s_t, a_t) + 1$</p></li><li><p><strong>Policy Evaluation</strong>: </p><p>$q(s_t, a_t) \leftarrow \text{Returns} (s_t, a_t) &#x2F; \text{Nums} (s_t, a_t)$</p></li><li><p><strong>Policy Improvement</strong>:</p><p>$\pi(a|s_t)&#x3D;1$ if $a &#x3D; arg \max_a q(s, a)$ and $\pi(a|s_t)&#x3D;0$ otherwise</p></li></ul></li></ul><h2 id="MC-epsilon-Greedy"><a href="#MC-epsilon-Greedy" class="headerlink" title="MC $\epsilon$-Greedy"></a>MC $\epsilon$-Greedy</h2><blockquote><p>Only if every state-action pair is well explored, their state-action values can be well estimated. This is the disavantage of both MC Basic and MC exploring start. Otherwise, if an action is not well explored, the action value may not be accurately estimated. The corresponding action may not be chosen even it is the optimal one. The fundamental idea of this Greedy Algorithm is to <strong>enhance exploration</strong> by balancing exploitation and exploration.</p></blockquote><h3 id="soft-policy"><a href="#soft-policy" class="headerlink" title="soft policy"></a>soft policy</h3><p>It has the <strong>positive probability</strong> to <strong>take any action at any state</strong>. With <em><strong>soft policy</strong></em>, a single episode that is <em><strong>sufficiently long</strong></em> can <em><strong>visit every state-action pair many times</strong></em>. Therefore, this idea to some degree achieves automation of generating a sequence of episodes that can visit every state-action pairs multiple times. </p><p><strong>The policy description</strong></p><p>The MC $\epsilon$-Greedy policy is <strong>a stochastic policy</strong> that has a <strong>higher chance of choosing the greedy action</strong> and the s<strong>ame nonzero probability of taking any other action</strong>. </p>$$\pi(a \mid s) =\begin{cases}1 - \dfrac{\epsilon}{|\mathcal{A}(s)|}\bigl(|\mathcal{A}(s)| - 1\bigr), & \text{for the greedy action}, \\[4pt]\dfrac{\epsilon}{|\mathcal{A}(s)|}, & \text{for other } |\mathcal{A}(s)| - 1 \text{ actions}.\end{cases}$$<p>When the $\epsilon$  is 0, this is totally greedy; when $\epsilon$  is 1, the probability of taking any action equals to $\frac{\epsilon}{|\mathcal{A(s)}|-1}$.</p><p>The <em><strong>Policy Improvement</strong></em> based on $\epsilon$-greedy. </p><p>$$<br>\pi_{k+1}(s) &#x3D; arg \max_{\pi \in \Pi_\epsilon} \sum_a \pi(a|s) q_{\pi_k}(s, a)<br>$$</p><p>The $\epsilon$-greedy policy is </p>$$\pi_{k+1}(a | s) =\begin{cases}1 - \dfrac{\epsilon}{|\mathcal{A}(s)|}\bigl(|\mathcal{A}(s)|-1\bigr), & a = a_k^*, \\\dfrac{\epsilon}{|\mathcal{A}(s)|}, & a \neq a_k^*\end{cases}$$<p><strong>Algorithm</strong></p><p><strong>Initialization</strong>: Initial Policy $\pi_0(a|s)$  and initial value $q(s, a)$ for all $(s, a)$.  $\text{Returns}(s, a)&#x3D;0$ and $\text{Nums}(s, a)&#x3D;0$.  </p><p><strong>Goal</strong>: Search for an optimal policy.</p><p><strong>Loop</strong>: For each episode, do</p><ul><li><p>Episode generation: Selecting a starting state-action pair $(s_0, a_0)$. Following the current policy, generate an episode of length T. </p></li><li><p>initialization for each episode: g ← 0</p></li><li><p>For each episode, $t &#x3D; T-1, T-2, \cdots, 0$, do</p><ul><li><p>$g \leftarrow \gamma g + r_{t+1}$</p></li><li><p>Returns $(s_t, a_t) \leftarrow$  Returns $(s_t, a_t) + g$</p></li><li><p>Nums $(s_t, a_t) \leftarrow$  Nums $(s_t, a_t) + 1$</p></li><li><p>Policy Evaluation:<br>$q(s_t, a_t) \leftarrow \text{Returns} (s_t, a_t) &#x2F; \text{Nums} (s_t, a_t)$</p></li><li><p>Policy Improvement:<br>$a^* &#x3D; arg \max_a q(s_t, a)$<br>The policy is </p>    $$    \pi(a \mid s) =    \begin{cases}    1 - \dfrac{\epsilon}{|\mathcal{A}(s)|}\bigl(|\mathcal{A}(s)| - 1\bigr), & a = a^*, \\[4pt]    \dfrac{\epsilon}{|\mathcal{A}(s)|}, & a \neq a^*    \end{cases}    $$    </li></ul></li></ul>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Reinforcement-Learning/">Reinforcement Learning</category>
      
      
      <category domain="http://example.com/tags/Reinforcement-Learning/">Reinforcement Learning</category>
      
      <category domain="http://example.com/tags/Monte-Carlo/">Monte Carlo</category>
      
      
      <comments>http://example.com/2025/11/09/RL-note3/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>RL Note 2: Value Iteration &amp; Policy Iteration</title>
      <link>http://example.com/2025/11/09/RL-note2/</link>
      <guid>http://example.com/2025/11/09/RL-note2/</guid>
      <pubDate>Sun, 09 Nov 2025 05:00:24 GMT</pubDate>
      
      <description>This note will explain vlaue interation &amp; policy iteration</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Value-Iteration-Policy-Iteration-Truncated-Iteration"><a href="#Value-Iteration-Policy-Iteration-Truncated-Iteration" class="headerlink" title="Value Iteration, Policy Iteration &amp; Truncated Iteration"></a>Value Iteration, Policy Iteration &amp; Truncated Iteration</h1><blockquote><p>The idea of interaction between value and policy update (<em>generalized policy iteration</em>) widely exists in <strong>Reinforcement Learning.</strong> This is also <strong>Dynamic Programming</strong>. These algorithm is also <strong>Model Required</strong>.</p></blockquote><h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><p>Solving the <strong>Bellman Optimality Equation</strong> through using contraction mapping theorem </p><p>$$<br>v_{k+1} &#x3D; \max_{\pi \in \Pi}(r_\pi + \gamma P_\pi v_k), \quad k &#x3D; 0, 1, 2, \dots<br>$$</p><p>This will guarantee the convergence of optimal state value and policy, according to the contract mapping theorem. </p><p><strong>Step 1</strong>: Policy Update</p><p>$$<br>\pi_{k+1} &#x3D; arg \max_\pi (r_\pi + \gamma P_\pi v_\pi)<br>$$</p><p><strong>Step2</strong>: Value Update</p><p>$$<br>v_{k+1}&#x3D;r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}}v_k<br>$$</p><p><strong>Elementwise Form Analysis</strong></p><p>First, find the best policy: </p><p>$$<br>\pi_{k+1}(s)&#x3D; arg\max_\pi\sum_{a\in \mathcal{A}}\pi(a|s)[\sum_{r\in \mathcal{R}}p(r|s, a)r+ \gamma \sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s, a)], \quad s\in \mathcal{S}<br>$$</p><p>Second, update the state value:</p><p>$$<br>v_{k+1} &#x3D; \sum_{a\in \mathcal{A}}\pi(a|s)[\sum_{r\in \mathcal{R}}p(r|s, a)r+ \gamma\sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s, a)], \quad s\in \mathcal{S}<br>$$</p><p>$a^* &#x3D; arg \max_a q_k(s, a)$, We have</p> $$\pi(a|s) = \begin{cases} 1, \quad a = a^*\\0, \quad a \neq a^*\end{cases}$$<p>Thus, the optimal state value after updating is<br>$$<br>v_{k+1}(s) &#x3D; \max_a q_k(s, a)<br>$$</p><p>where the $v_k, v_{k+1}$ are only the intermediate value generated in the process of algorithm. </p><p><strong>Algorithm</strong></p><ul><li><p><strong>Initialization:</strong> The probability models $p(r\mid s,a)$ and $p(s’\mid s,a)$ for all $(s,a)$ are known.</p></li><li><p><strong>Goal:</strong> Search the optimal state values and an optimal policy that satisfy the Bellman optimality equation.</p></li><li><p><strong>Loop:</strong> While $\lVert v_k - v_{k+1} \rVert_\infty \ge \varepsilon$ (iteration $k$):</p><ul><li><p>For every state $s \in \mathcal{S}$:</p><ul><li><p>For every action $a \in \mathcal{A}$:</p><ul><li>Step 1:          $$        q_k(s,a)        = \sum_{r\in\mathcal{R}} p(r\mid s,a)\, r        + \gamma \sum_{s'\in\mathcal{S}} p(s'\mid s,a)\, v_k(s')        $$        </li></ul></li><li><p>Step 2: Choose greedy action<br>$$<br>a_k^*(s) &#x3D; \arg\max_a q_k(s,a)<br>$$</p></li><li><p>Step 3: Policy update</p>      $$      \pi_{k+1}(a\mid s) =      \begin{cases}      1, & a = a_k^*(s),\\      0, & a \ne a_k^*(s)      \end{cases}      $$      </li><li><p>Step 4: Value update</p></li></ul><p>$$<br>v_{k+1}(s) &#x3D; \max_a q_k(s,a)<br>$$</p></li></ul></li></ul><h2 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h2><blockquote><p>Indirectly solving the Bellman Equation. It’s the foundation of <em><strong>Monte Carlo (MC)</strong></em>. It has two parts, policy evaluation and <strong>policy improvement</strong>. </p></blockquote><p><strong>Step1</strong>: Policy Evaluation. </p><p>This step will calculate the state value and evaluate the policy. </p><p>$$<br>v_{\pi_k} &#x3D; r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}<br>$$</p><p>where the $r_{\pi_k}, P_{\pi_k}$ can be obtained through system models, $\pi_k$ is the policy obtained in the last iteration and $v_{\pi_k}$ is the state value to be calculated in this iteration. </p><p><strong>Step2:</strong> Policy Improvement.</p><p>once $v_<br>{\pi_k}$ is calculated at this first step, we can update the policy $\pi_{k+1}$ through.<br>$$<br>\pi_{k+1} &#x3D; arg\max_\pi(r_\pi + \gamma P_\pi v_{\pi_k})<br>$$</p><p><strong>How to obtain</strong> $v_{\pi_k}$ 🤔?</p><p>To calculate $v_{\pi_k}$, we need to do an <strong>embedded iteration</strong> based on <strong>contract mapping theorem</strong>. We can also use $v_{\pi_k}&#x3D;(I-\gamma P_{\pi_k})^{-1}r_{\pi_k}$  and find the inverse of matrix. But it is not recommended. </p><p>$$<br>v_{\pi_k}^{(j+1)} &#x3D; r_{\pi_k} + \gamma P_{\pi_k}v_{\pi_k}^{(j)}, \quad j &#x3D; 0, 1, 2, \cdots<br>$$</p><p>Mathematically, there will be infinite steps to obtain the convergence. However, we can artificially set a threshold for $||v_{\pi_k}^{(j+1)} - v_{\pi_k}^{(j)}||$ or the maximum steps for $j$. </p><p><strong>Why is $\pi_{k+1}$  better than $\pi_k$</strong>?<br>$$<br>\because \pi_{k+1} &#x3D; arg\max_\pi(r_\pi + \gamma P_\pi v_{\pi_k})<br>$$</p><p>$$<br>\therefore v_{\pi_{k+1}}(s) \geq v_{\pi_k}(s) , \quad \text{for all}\quad s<br>$$</p><p> The prove is that since $v_{\pi_{k+1}}$ and $v_{\pi}$ are states values, they satisfy the Bellman Equation.<br>$$<br>v_{\pi_{k+1}} &#x3D; r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k+1}}<br>$$</p><p>$$<br>v_{\pi_{k}} &#x3D; r_{\pi_{k}} + \gamma P_{\pi_{k}} v_{\pi_{k}}<br>$$</p><p>Because equation (15), we  know that $r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k}} &gt; r_{\pi_{k}} + \gamma P_{\pi_{k}} v_{\pi_{k}}$. Therefore<br>$$<br>v_{\pi_{k}} - v_{\pi_{k+1}}&#x3D; (r_{\pi_{k}} + \gamma P_{\pi_{k}}  v_{\pi_{k}}) - (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k+1}}) \leq (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}}  v_{\pi_{k}}) - (r_{\pi_{k+1}} + \gamma P_{\pi_{k+1}} v_{\pi_{k+1}}) &#x3D; \gamma P_{\pi_{k+1}}(v_{\pi_k} - v_{\pi_{k+1}})<br>$$<br>Thus, $v_{\pi_{k}} - v_{\pi_{k+1}} \leq \lim_{n\rightarrow \infty} \gamma^n P_{\pi_{k+1}}^n(v_{\pi_{k}} - v_{\pi_{k+1}})&#x3D;0$</p><blockquote><p>Since the policy iteration contains an embedded iterations, its convergence will be faster than value iteration.</p></blockquote><p><strong>Algorithm</strong></p><p><strong>Initialization</strong>: The system model, $p(r|s, a)$ and $p(s’|s,a)$ for all $s, a$ is known. Initial guess $\pi_0$</p><p><strong>Goal</strong>: Search for optimal state value and an opitmal policy. </p><p><strong>Loop</strong>: While $v_{\pi_k}$ has not converged, for the $k$ th iteration,  do</p><ul><li><p><strong>Policy Evaluation</strong>:</p><p>Initialization: an arbitrary initial guess $v_{\pi_k}^{(0)}$</p><p>While $v_{\pi_k}^{(j)}$ has not converged, for the $j$ th iteration, do </p><ul><li>For every state:</li><li>$v_{\pi_k}^{(j+1)}(s) &#x3D; \sum_a \pi_k(a|s) [\sum_r p(r|s, a)r + \gamma\sum_{s’}p(s’|s, a)v_{\pi_k}^{(j)}(s’)]$</li></ul></li><li><p><strong>Policy Improvement</strong>:</p><p>For every state $s \in \mathcal{S}$, do:</p><ul><li>For every action $a \in \mathcal A$, do:<ul><li>Calculate the action value:</li><li>$q_{\pi_k}(s, a) &#x3D; \sum_r p(r|s, a)r + \gamma\sum_{s’}p(s’|s, a)v_{\pi_k}(s’)$</li></ul></li><li>$a_k^*(s) &#x3D; arg\max_a q_{\pi_k} (s, a)$</li><li>$\pi_{k+1}(a|s) &#x3D; 1$  if $a = a^*_k$ and $\pi_{k+1}(a|s)=0$ otherwise. </li></ul></li></ul><p>​</p><p>In the numerical experiment, </p><ul><li>we can find that the states that are close to the target area can find the optimal policy <em><strong>faster than those far away</strong></em>.</li><li>we can also find that the <em><strong>states that are closed to the target area</strong></em> have larger <em><strong>state values</strong></em>.</li></ul><h2 id="Truncated-Policy-Iteration"><a href="#Truncated-Policy-Iteration" class="headerlink" title="Truncated Policy Iteration"></a>Truncated Policy Iteration</h2><p>It is the unified version of Value Iteration and Policy Iteration. </p><p>​<img src="/2025/11/09/RL-note2/figure3.png" class=""></p><p>​<img src="/2025/11/09/RL-note2/figure4.png" class=""></p><ul><li>If the embedded iteration is only <em><strong>one-step</strong></em> instead of <em><strong>infinite steps</strong></em> then the <em><strong>policy iteration</strong></em> will be degraded to <em><strong>value iteration</strong></em>.</li><li>If the embedded iteration is <em><strong>finite steps</strong></em> instead of <em><strong>infinite steps</strong></em> then the <em><strong>policy iteration</strong></em> will be degraded to <em><strong>truncated policy iteration</strong></em>.</li></ul>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Reinforcement-Learning/">Reinforcement Learning</category>
      
      
      <category domain="http://example.com/tags/Value-Iteration/">Value Iteration</category>
      
      <category domain="http://example.com/tags/Policy-Iteration/">Policy Iteration</category>
      
      
      <comments>http://example.com/2025/11/09/RL-note2/#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>RL Note 1: Basic Concepts &amp; Tools for RL</title>
      <link>http://example.com/2025/11/09/RL-note1/</link>
      <guid>http://example.com/2025/11/09/RL-note1/</guid>
      <pubDate>Sun, 09 Nov 2025 04:11:01 GMT</pubDate>
      
      <description>This note will explain baisc concepts of RL and Bellman Equation.</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Key-Terms"><a href="#Key-Terms" class="headerlink" title="Key Terms"></a>Key Terms</h1><ul><li><strong>Agent</strong>: the object we are observing for finding the best optimal policies.</li><li><strong>States</strong>: $s_i$ The <strong>state space</strong> is $\mathcal{S} &#x3D; {s_1, \cdots s_n}$</li><li><strong>Actions</strong>: $a_i$ The <strong>action space</strong> is $\mathcal{A} &#x3D; {a_1,\cdots, a_m}$</li><li><strong>State Transition</strong>: $s_1 \xrightarrow{a_2} s_2$</li><li><strong>Rewards</strong>: the feedback from the environment. <strong>Positive reward</strong> will encourage an agent to take the corresponding action, and vice versa for <strong>negative reward</strong>. (Human-Machine Interface)</li><li><strong>Returns</strong>: the <strong>return of trajectory</strong> is the <strong>sum of rewards</strong> collected along the trajectory. (Also known as <em>Total rewards or cumulative reward</em>). Returns consists <em><strong>immediate reward</strong></em> and <em><strong>future reward</strong></em>.</li><li><strong>Discounted Returns</strong>: A discounted factor is applied for infinitely long trajectories. It can also <strong>adjust the emphasis on near- or far-future rewards</strong>.</li><li><strong>Policies</strong>: Policy (<strong>stochastic</strong> in general, but sometimes <strong>deterministic</strong>) tells the agent which actions to take at every state, which can be denoted as arrow.</li><li><strong>Trajectory</strong>: a state-action-reward chain. $s_1 \xrightarrow[r&#x3D;0]{a_2} s_2   \xrightarrow[r&#x3D;0]{a_3} s_5   \xrightarrow[r&#x3D;0]{a_3} s_8   \xrightarrow[r&#x3D;1]{a_2} s_9 .$ There is <em><strong>infinite-length(horizon) trajectory</strong></em> except for <em><strong>finite trajectory</strong></em>. So for <em><strong>finite returns</strong></em> and <em><strong>infinite reward.</strong></em></li><li><strong>Immediate Rewards</strong>: reward obtained after taking an action at the initial state.</li><li><strong>Future Rewards</strong>: reward obtained after leaving the initial state.</li><li><strong>Episodes</strong>: a finite trajectory with terminal states whose result trajectory is an episode.</li><li><strong>episodic task&#x2F;continuing task</strong>: whether terminal states exist. The episodic tasks can be mathematically converted to continuing tasks by defining the processes after reaching terminal states.</li><li><strong>visit</strong>: every time a state-action pair appears in an episode, it is called a <em><strong>visit</strong></em> of <strong>state-action pair</strong>.</li><li><strong>Behavior Policy</strong>: it is used to generate experience samples.</li><li><strong>Target Policy</strong>: it is constantly updated to converge to an optimal policy.</li><li><strong>On-policy</strong>: When the <strong>behavior policy</strong> is equal to the <strong>target policy</strong>.</li><li><strong>Off-policy</strong>: When the <strong>behavior policy</strong> is different from the <strong>target policy</strong>. off-policy learning can learn optimal policies based on the experience samples generated by other policies.</li><li><strong>Experience replay:</strong> <em>The draw of samples is called <strong>experience replay</strong>, which should follow a uniform distribution.</em> The main function of experience replay is to break the correlation of a sequence of action-state pairs. <strong>After</strong> we have collected some experience samples, we do not use these samples in the order they were collected. They are stored at the <strong>replay buffer</strong>. $(s, a, r, s’)$ is the experience sample and $\mathcal{B}&#x3D; {(s, a, r, s’)}$ is the replay buffer. Every time updating the network we will draw a mini-batch from the <strong>replay buffer</strong>.</li><li><strong>Policy Evaluation</strong>: <em><strong>Policy Evalution is the process calculating state value or action value based on given policy</strong> when the system dynamics is given or samplable.</em> Since the state value can be used to evaluate a policy, solving the state values from the Bellman Equation is <em><strong>policy evaluation</strong></em>. Basically, the task of Policy Evalution is to solve the Bellman Equation.</li></ul><p>Two ways to express <strong>Deterministic</strong> <strong>State Transition</strong>:</p><ol><li>Table (<strong>only</strong> <strong>deterministic state transition</strong>)</li></ol><p>​<img src="/2025/11/09/RL-note1/figure1.png" class="" title="figure1"></p><ol><li><p>Probability Expression</p> <img src="/2025/11/09/RL-note1/figure2.png" class="" title="figure2"></li></ol><h1 id="Key-Concept-Key-Tool"><a href="#Key-Concept-Key-Tool" class="headerlink" title="Key Concept &amp; Key Tool"></a>Key Concept &amp; Key Tool</h1><h2 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h2><h3 id="State-Value"><a href="#State-Value" class="headerlink" title="State Value"></a>State Value</h3><blockquote><p>It’s the <strong>expected return</strong> (<strong>average reward</strong> &#x2F; <strong>total reward</strong>) that an agent can obtain when <strong>starting from a state</strong> through <strong>following a given policy</strong>.  It can be taken as a metric to evaluate the goodness of a policy.</p></blockquote><p>The returns can be utilized to evaluate policies. However, it is inapplicable to stochastic systems, because starting from one state will lead to different returns. </p><p>Therefore,   <strong>returns → state values</strong></p><p>$$<br>S_t \xrightarrow{A_t} S_{t+1}, R_{t+1}<br>$$</p><p>$S_t, A_t, S_{t+1}, R_{t+1}$ are all random variables.  $A_t \in \mathcal{A}(S_t)$  is the action taken by following Policy $\pi$. The immediate reward is $R_{t+1} \in \mathcal{R}(S_t,A_t)$. Therefore, the <strong>discounted return</strong> along the trajectory is </p><p>$$<br>G_t &#x3D; R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3}+ \cdots,<br>$$</p><p>where the $G_t$ is a <em><strong>random variable</strong></em>, which can be obtained by calculating <em><strong>expected value</strong></em>. Since the policy is given, $A_t$ is not a random variable here. </p><p>$$<br>v_\pi(s) &#x3D; \mathbb{E}[G_t|S_t &#x3D; s]<br>$$</p><p>This expected value $v_\pi (s)$  is called the <em><strong>state-value function</strong></em> or <em><strong>state value</strong></em> of $s$ .</p><h3 id="Motivating-Example"><a href="#Motivating-Example" class="headerlink" title="Motivating Example:"></a>Motivating Example:</h3><p>​<img src="/2025/11/09/RL-note1/figure3.png" class="" title="figure3"></p><p>The return by following the first policy is $\text{return}_{1} &#x3D; 0 + \gamma1 + \gamma 1 + \cdots &#x3D; \frac{\gamma}{1 - \gamma}$;</p><p>The return by following the second policy is $\text{return}_2 &#x3D; -1 + \frac{\gamma}{1- \gamma}$;</p><p>The thrid return is $\text{return}_3 &#x3D; 0.5 \times \text{return}_1 +  0.5 \times \text{return}_2  &#x3D; -0.5  + \frac{\gamma}{1- \gamma}$</p><h3 id="The-idea-to-calculate-returns"><a href="#The-idea-to-calculate-returns" class="headerlink" title="The idea to calculate returns:"></a>The idea to calculate returns:</h3><p>​</p><p>$$<br>v_1 &#x3D; r_1 + \gamma (r_2 + \gamma r_3 + \cdots) &#x3D; r_1 + \gamma v_2<br>$$</p><p>$$<br>v_2 &#x3D; r_2 + \gamma (r_3 + \gamma r_4 + \cdots) &#x3D; r_2 + \gamma v_3<br>$$</p><p>$$<br>v_3 &#x3D; r_3 + \gamma (r_4 + \gamma r_1 + \cdots) &#x3D; r_3 + \gamma v_4<br>$$</p><p>$$<br>v_4 &#x3D; r_4 + \gamma (r_1 + \gamma r_2 + \cdots) &#x3D; r_4 + \gamma v_1<br>$$</p><p>The idea is <em><strong>bootstrapping:</strong></em> using current estimates to update futrue estimates .  It is also an endless loop because the calculation of an unknown value relies on another unknown value. </p><h3 id="Action-Value"><a href="#Action-Value" class="headerlink" title="Action Value"></a>Action Value</h3><blockquote><p><strong>Action value</strong> indicates the value (<em><strong>expected return</strong></em>) of taking an action at a state. The action value is highly dependent on <em><strong>State Value</strong>.</em> It is also known as <em><strong>state-action value</strong></em>. Action value and state value can be converted to each other.</p></blockquote><p>The action value of a state-action pair can be defined as </p><p>$$<br>\begin{aligned}<br>q_\pi(s,a)<br>&amp;&#x3D; \mathbb{E}\big[ G_t \mid S_t&#x3D;s, A_t&#x3D;a \big] \<br>&amp;&#x3D; \sum_{r \in \mathcal{R}} p(r \mid s,a), r</p><ul><li>\sum_{s’ \in \mathcal{S}} p(s’ \mid s,a), v_\pi(s’)<br>\end{aligned}<br>$$</li></ul><p><em><strong>This shows how to obtain action values through using state values.</strong></em><br>$$<br>\because \mathbb{E}[G_t|S_t &#x3D; s] &#x3D; \sum_{a\in \mathcal{A}}\pi(a|s)[\sum_{r\in \mathcal{R}}p(r|s, a)r+ \sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s, a)]  \ &#x3D;  \sum_{a\in \mathcal{A}}\mathbb{E}[G_t|S_t&#x3D;s, A_t&#x3D;a]\pi(a|s)<br>$$</p><p><strong>This shows how to obtain state values through using action values.</strong><br>$$<br>v_\pi(s) &#x3D; \sum_{a\in \mathcal{A}}\pi(a|s)q_\pi(s,a)<br>$$</p><h3 id="Optimal-Policy"><a href="#Optimal-Policy" class="headerlink" title="Optimal Policy"></a>Optimal Policy</h3>The state values of optimal policy is the greatest compared to other policies. It can also be written as Matrix Form.$$v^*(s) = \max_{\pi} v^\pi(s),\quadv^{\pi^*}(s) \ge v^\pi(s),\ \forall s,\pi.$$<h3 id="Optimal-State-Value"><a href="#Optimal-State-Value" class="headerlink" title="Optimal State Value"></a>Optimal State Value</h3>We can obtain the optimal policies based on optimal state value that solved from Bellman Optimality Equation. A policy $\pi^*$  is optimal if $v_{\pi^*}(s)> v_\pi (s)$ for all $s \in \mathcal{S}$ and for any other policy $\pi$. The state values of $\pi^*$ are the optimal state values. Therefore, an optimal policy has the greatest state value for every state compared to any other policies. The Bellman optimality equation can be written as $$v^*(s) = \max_a[r(s, a),\gamma \sum_{s'}P(s'|s, a)v^*(s')]$$<h2 id="Tools"><a href="#Tools" class="headerlink" title="Tools"></a>Tools</h2><h3 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h3><blockquote><p>Bellman Equation is basically a set of linear equations that describe the relationship between values of all the states. Bellman equation can be used to analyze the state values. It describes the relationship between values of all states.</p></blockquote><p>The below is the return from decision epoch $t$. </p><p>$$<br>G_t &#x3D; R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \cdots ) &#x3D; R_{t+1} + \gamma G_{t+1}<br>$$</p><p>Therefore, the state value we determined before can be written as</p><p>$$<br>v_{\pi}(s) &#x3D; \mathbb{E}[G_t|S_t &#x3D; s]&#x3D; \mathbb{E}[R_{t+1}+ \gamma G_{t+1}] &#x3D; \mathbb{E}[R_{t+1}|S_{t}&#x3D;s] + \gamma \mathbb{E}[G_{t+1}|S_t &#x3D; s]<br>$$</p><p>$$<br>\mathbb{E}[R_{t+1}|S_t &#x3D; s] &#x3D; \sum_{a\in \mathcal{A}}\pi(a|s) \mathbb{E}[R_{t+1}|S_{t+1}&#x3D;s, A_t&#x3D;a] &#x3D; \sum_{a\in \mathcal{A}} \pi(a|s) \sum_{r\in \mathcal{R}}p(r|s, a)r<br>$$</p><p>$$<br>\mathbb{E}[G_{t+1}|S_t &#x3D;s]&#x3D; \sum_{s’\in \mathcal{S}}\mathbb{E}[G_{t+1}|S_t &#x3D; s, S_{t+1}&#x3D;s’]p(s’|s)&#x3D; \sum_{s’\in \mathcal{S}}\mathbb{E}[G_{t+1}|  S_{t+1}&#x3D;s’]p(s’|s) &#x3D; \sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s) &#x3D;\sum_{s’\in \mathcal{S}}v_\pi(s’)\sum_{a\in \mathcal{A}}\pi(a|s)p(s’|s, a)<br>$$</p><p>The Bellman Equation will be</p><p>$$<br>v_\pi(s) &#x3D; \mathbb{E}[G_t|S_t&#x3D;s] &#x3D; \mathbb{E}[R_{t+1}|S_t&#x3D;s] + \gamma \mathbb{E}[G_{t+1}|S_t&#x3D;s] &#x3D; \sum_{a\in \mathcal{A}} \pi(a|s) \sum_{r\in \mathcal{R}}p(r|s, a)r +\sum_{s’\in \mathcal{S}}v_\pi(s’)\sum_{a\in \mathcal{A}}\pi(a|s)p(s’|s, a) &#x3D; \sum_{a\in \mathcal{A}}\pi(a|s)[\sum_{r\in \mathcal{R}}p(r|s, a)r +\sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s, a)]<br>$$</p><p>the Bellman Equation is written as the sum of <em><strong>mean of immediate rewards</strong></em> and <em><strong>mean of future rewards</strong></em>. In the equation, the probability models, policies, rewards is known or given. </p><p>​<img src="/2025/11/09/RL-note1/figure5.png" class="" title="figure5"></p><p>​<img src="/2025/11/09/RL-note1/figure6.png" class="" title="figure6"></p><p>​<img src="/2025/11/09/RL-note1/figure7.png" class="" title="figure7"></p><p>The above derivations and results are all elementwise form, there are also matrix-vector form. </p><h3 id="Matrix-Vector-Form"><a href="#Matrix-Vector-Form" class="headerlink" title="Matrix-Vector Form"></a>Matrix-Vector Form</h3><p>$$<br>v_\pi &#x3D; r_\pi + \gamma P_\pi v_\pi<br>$$</p><p>​<img src="/2025/11/09/RL-note1/figure8.png" class="" title="figure8"></p><p>The solution is $v_\pi &#x3D; (I-\gamma P_\pi)^{-1}r_\pi$, where $(I-\gamma P_\pi)$ is invertible, and each element of $(I-\gamma P_\pi)^{-1}$ is no less than the identity matrix. </p><p>However, to obtain this solution, we have to calculate the inverse matrix. Instead, we usually apply iterative solution. </p><p>$$<br>v_{k+1} &#x3D; r_{\pi} +\gamma P_\pi v_k, \quad k &#x3D; 0, 1, 2, \cdots<br>$$</p><p>  where the $v_0$ is the initial guess of $v_\pi$.</p><h3 id="Bellman-Equation-in-terms-of-Action-Value"><a href="#Bellman-Equation-in-terms-of-Action-Value" class="headerlink" title="Bellman Equation in terms of Action Value"></a>Bellman Equation in terms of Action Value</h3><p>$$<br> v_k \rightarrow v_\pi &#x3D; (I-\gamma P_\pi)^{-1}r_\pi, \quad \text{as}\quad  k  \rightarrow \infty<br>$$</p><p>$$<br>q_\pi(s, a)&#x3D;\sum_{r\in \mathcal{R}}p(r|s, a)r+ \sum_{s’\in \mathcal{S}}v_\pi(s’)p(s’|s, a)<br>$$</p><p>and </p><p>$$<br>v_\pi (s) &#x3D; \sum_{a\in \mathcal{A}}\pi(a|s)q_\pi(s, a)<br>$$</p><p>then, the Bellman Equation becomes. </p><p>$$<br>q_\pi(s, a)&#x3D;\sum_{r\in \mathcal{R}}p(r|s, a)r+ \sum_{s’\in \mathcal{S}}p(s’|s, a)\sum_{a’\in \mathcal{A(s’)}}\pi(a’|s’)q_\pi(s’, a’)<br>$$</p><h3 id="Bellman-Optimality-Equation"><a href="#Bellman-Optimality-Equation" class="headerlink" title="Bellman Optimality Equation"></a>Bellman Optimality Equation</h3><p>This equation is used to obtain the optimal policies, that is, we can use Bellman Optimality Equation to solve the <strong>optimal state values and policies</strong>. </p><h3 id="How-to-improve-policies"><a href="#How-to-improve-policies" class="headerlink" title="How to improve policies?"></a>How to improve policies?</h3><p>Mathematics: The problem can be realized based on the calculation of <em><strong>state values</strong></em> and <em><strong>action values</strong></em>.  </p><p>First, we can calculate the state values based on the given policy, like this: </p><p>​<img src="/2025/11/09/RL-note1/figure8.png" class="" title="figure8"></p><p>Second, we can calculate the <strong>action values</strong> for each state. Here we only take state $s_1$ as an example. </p><blockquote><p>Here we have to emphasize the concept of action value again: it is the expected returns obtain at a given state by conducting a certain action.</p></blockquote><p>​<img src="/2025/11/09/RL-note1/figure9.png" class="" title="figure9"></p><p>Obviously, $a_3$ is the greatest action. </p><p>For every $s \in \mathcal{S}$ , the <strong>elementwise expression</strong> of the BOE (a set of linear equation for updating state value) is </p><p>$$<br>v(s) &#x3D; \max_{\pi(s) \in \Pi(s)} \sum_{a\in \mathcal{A}}\pi(a|s) (\sum_{r\in\mathcal{R}}p(r|s, a)r + \gamma \sum_{s’\in \mathcal{S}}p(s’|s, a)v(s’)) &#x3D; \max_{\pi(s) \in \Pi(s)}\sum_{a\in\mathcal{A}}\pi(a|s)q(s, a)<br>$$</p><p>$\pi(s)$ is a policy for state $s$, $\Pi(s)$ is the set of all possible policies for $s$</p><p>Although the BOE has two unknown variables $v(s)$ and $\pi(a|s)$, they can be solved one by one. </p><p>Because of the structure of this Equation, we can be inspired by the below example!</p><p>​</p><p>Here we have $\sum_a \pi(a|s) &#x3D; 1$, we have</p><p>$$<br>\sum_{a\in \mathcal{A}}\pi(s|a)q(s, a) &#x3D; \sum_{a\in \mathcal{A}}\pi(s|a) \max_{a\in \mathcal{A}} q(s, a) &#x3D; \max_{a\in \mathcal{A}}q(s, a)<br>$$</p><p>where equality is achieved when </p><h3 id="Greedy-Optimal-Policy"><a href="#Greedy-Optimal-Policy" class="headerlink" title="Greedy Optimal Policy"></a>Greedy Optimal Policy</h3>this is an optimal policy for solving the BOE$$\pi(a|s) = \begin{cases} 1, \quad a = a^*\\0, \quad a \neq a^*\end{cases}$$The matrix-vector form of BOE is $$v = \max_{\pi \in \Pi}(r_\pi + \gamma P_\pi v)$$it can be expressed as $$f(v) = \max_{\pi \in \Pi}(r_\pi + \gamma P_\pi v)$$therefore, we only need to achieve $v = f(v)$, here we have to use Contraction Mapping Theorem (Fixed Pointed Theorem). Consider a function $f(x)$, where $x \in \mathbb{R}^d$ and $f: \mathbb{R}^d \rightarrow \mathbb{R}^d$. The point $x^*$ is called fixed point if $f(x^*) = x^*$. Therefore the function $f$ is the contraction mapping, if there exists $\gamma \in(0, 1)$ $$||f(x_1)-f(x_2)|| \leq \gamma ||x_1 - x_2||$$<p><strong>Theorem 3.1</strong> (Contraction Mapping Theorem). For any equation that has the form $x &#x3D;f(x)$ where $x$ and $f(x)$ are real vectors, if $f$ is a contraction mapping, then the following properties hold. </p><ul><li>Existence:   There exists a fixed point $x^*$ satisfying  $f(x^*)=x^*$. </li><li>Uniqueness: The fixed point $x^*$ is unique.</li><li>Algorithm: Consider the iterative process: $x_{k+1} &#x3D; f(x_k)$</li></ul>where $x_k \rightarrow x^*$, $k \rightarrow \infty$ for any initial guess $x_0$.  The function $f(v)$ on the right-hand side of the BOE in (3.3) is a contraction mapping. For any $v_1, v_2 \in \mathbb{R}^{|S|}$, it holds that$$||f(v_1)-f(v_2)||_\infty \leq \gamma ||v_1 -v_2||_\infty$$here we use the maximum norm. <h3 id="solving-optimal-state-values"><a href="#solving-optimal-state-values" class="headerlink" title="solving optimal state values"></a>solving optimal state values</h3>Therefore, for the BOE $v = f(v) = \max_{\pi \in \Pi}(r_\pi + \gamma P_\pi v^*)$. $v^*$ is the fixed point. There always exists a unique solution $v^*$, which can be solved iteratively by $$v_{k+1} = f(v_k)= \max_{\pi \in \Pi}(r_\pi + \gamma P_\pi v_k)$$<h3 id="solving-optimal-policy"><a href="#solving-optimal-policy" class="headerlink" title="solving optimal policy"></a>solving optimal policy</h3>after we obtain the optimal state values $v^*$, we can easily obtain $\pi^*$ by solving $$\pi^* = arg\max_{\pi\in \Pi} (r_\pi + \gamma P_\pi v^*)$$<p>Then </p>$$v^* = r_{\pi^*} + \gamma P_{\pi^*}v^*$$<h3 id="Greedy-Optimal-Policy-1"><a href="#Greedy-Optimal-Policy-1" class="headerlink" title="Greedy Optimal Policy"></a>Greedy Optimal Policy</h3><p>For any $s \in \mathcal{S}$, the <em><strong>deterministic greedy policy</strong></em></p>$$\pi(a|s) = \begin{cases} 1, \quad a = a^*\\0, \quad a \neq a^*\end{cases}$$Here, $a^*(s) = arg \max_a q^*(a, s)$ , where$$q^*(s, a) = \sum_{r\in \mathcal{R}}p(r|s, a)r + \gamma \sum_{s' \in \mathcal{S}}p(s'|s, a)v^*(s')$$]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Reinforcement-Learning/">Reinforcement Learning</category>
      
      
      <category domain="http://example.com/tags/Reinforcement-Learning/">Reinforcement Learning</category>
      
      <category domain="http://example.com/tags/Bellman-Equation/">Bellman Equation</category>
      
      
      <comments>http://example.com/2025/11/09/RL-note1/#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
